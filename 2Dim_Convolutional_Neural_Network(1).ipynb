{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceREbGEG8r2S"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dtKbauqHgu3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0_Yp8s0UHhfV"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(x, y), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h30WbMSjHh2Q",
    "outputId": "3d97da33-8952-45d2-e085-581c0ae91c0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000, 28, 28)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x.shape)\n",
    "print(x[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zrW7neavHiCd",
    "outputId": "48ecb195-fd32-4f2e-fea9-c62ee832b7be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-99-e2e56b88b339>:1: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x = x.astype(np.float)\n",
      "<ipython-input-99-e2e56b88b339>:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x_test = x_test.astype(np.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "x = x.astype(np.float)\n",
    "x_test = x_test.astype(np.float)\n",
    "x /= 255\n",
    "x_test /=255\n",
    "print(x.max())\n",
    "print(x.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cz7N4NcSyAI",
    "outputId": "3b23b6fd-6aa3-47fb-9cc5-480e79d65058"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(60000, 10)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_one_hot = enc.fit_transform(y[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "print(y.shape)\n",
    "print(y_one_hot.shape)\n",
    "print(y_one_hot.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99ILg-zJSyyP",
    "outputId": "dd534a42-bb74-4a85-e866-7795fae7ebb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 28, 28)\n",
      "(12000, 28, 28)\n",
      "(48000, 10)\n",
      "(12000, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x, y_one_hot, test_size=0.2)\n",
    "print(x_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aWhkPvlvUcOv"
   },
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    Fully connected layers from number of nodes n_nodes1 to n_nodes2\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      Number of nodes in the previous layer\n",
    "    n_nodes2 : int\n",
    "      Number of nodes in subsequent layers\n",
    "    initializer : Instances of initialization methods\n",
    "    optimizer : Instances of optimization methods\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, activation):\n",
    "        \n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "        # Initialize.\n",
    "        # Use the initializer method to initialize self.W and self.B\n",
    "        self.W = self.initializer.W(self.n_nodes1,self.n_nodes2)\n",
    "        self.B = self.initializer.B(self.n_nodes2)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of the following form, shape (batch_size, n_nodes1)\n",
    "            Input\n",
    "        Returns\n",
    "        ----------\n",
    "        A : ndarray of the following form, shape (batch_size, n_nodes2)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.A = np.dot(self.X,self.W) + self.B\n",
    "        \n",
    "        return self.activation.forward(self.A)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        Backward\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray of the following form, shape (batch_size, n_nodes2)\n",
    "            The gradient flowed in from behind.\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : ndarray of the following form, shape (batch_size, n_nodes1)\n",
    "            forward slope\n",
    "        \"\"\"\n",
    "        dA = self.activation.backward(dZ)\n",
    "        self.dB = np.mean(dA,axis=0)\n",
    "        self.dW = np.dot(self.X.T,dA)/len(self.X)\n",
    "        dZ = np.dot(dA,self.W.T)\n",
    "        \n",
    "        # Update\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrNUEezVXjbY"
   },
   "outputs": [],
   "source": [
    "class SimpleInitializerConv2d:\n",
    "    \"\"\"\n",
    "    Simple initialization with Gaussian distribution\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      Standard deviation of Gaussian distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma=0.01):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, F, C, FH, FW):\n",
    "        \"\"\"\n",
    "        Initializing weights\n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        W : weight\n",
    "        \"\"\"\n",
    "        return self.sigma * np.random.randn(F,C,FH,FW)\n",
    "    \n",
    "    def B(self, F):\n",
    "        \"\"\"\n",
    "        Bias initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : bias\n",
    "        \"\"\"\n",
    "        return np.zeros(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5cX1r1pXjyA"
   },
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    Simple initialization with Gaussian distribution\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      Standard deviation of Gaussian distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initializing weights\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          Number of nodes in the previous layer\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in subsequent layers\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : weight\n",
    "        \"\"\"\n",
    "        return self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Bias initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in subsequent layers\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : bias\n",
    "        \"\"\"\n",
    "        return np.zeros(n_nodes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2CdbnvDaRA1"
   },
   "outputs": [],
   "source": [
    "class HeInitializer():\n",
    "    \"\"\"\n",
    "    Initialization of weights by He\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initializing weights\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          Number of nodes in the previous layer\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in subsequent layers\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : weight\n",
    "        \"\"\"\n",
    "        return np.random.randn(n_nodes1, n_nodes2)*np.sqrt(2/n_nodes1)\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Bias initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in subsequent layers\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : bias\n",
    "        \"\"\"\n",
    "        return np.zeros(n_nodes2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaimgH3DZ1zk"
   },
   "source": [
    "Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFFpSj_OXkI-"
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    stochastic gradient descent method\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : learning rate\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Updating the weights and biases of a layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : An instance of the layer before the update\n",
    "        \"\"\"\n",
    "        layer.W -= self.lr*layer.dW\n",
    "        layer.B -= self.lr*layer.dB\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FN16gmo6b2Og"
   },
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    stochastic gradient descent method\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : learning rate\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.hW = 0\n",
    "        self.hB = 0\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Updating the weights and biases of a layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : An instance of the layer before the update\n",
    "        \"\"\"\n",
    "        self.hW += layer.dW*layer.dW\n",
    "        self.hB = layer.dB*layer.dB\n",
    "    \n",
    "        layer.W -= self.lr*layer.dW/(np.sqrt(self.hW) +1e-7)\n",
    "        layer.B -= self.lr*layer.dB/(np.sqrt(self.hB) +1e-7)\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEs0rZWkf6_J"
   },
   "source": [
    "Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0CDl8MRb2ns"
   },
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    \"\"\"\n",
    "    Activation function : ReLU function\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self,A):\n",
    "        self.A = A\n",
    "        return np.maximum(self.A,0)\n",
    "    \n",
    "    def backward(self,dZ):\n",
    "        \n",
    "        return np.where(self.A>0,dZ,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekjt9tETb2zo"
   },
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    \"\"\"\n",
    "    Activation Function : Softmax Function\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self,A):\n",
    "        \n",
    "        return np.exp(A-np.max(A))/np.sum(np.exp(A-np.max(A)),axis=1,keepdims=True)\n",
    "    \n",
    "    def backward(self,dZ):\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Osrrb2qog2No"
   },
   "source": [
    "Mini Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yzYYK-ag1Ck"
   },
   "outputs": [],
   "source": [
    "# Mini-batch processing class\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    Iterator to get the mini-batch\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray of the following form, shape (n_samples, n_features)\n",
    "      Training data\n",
    "    y : ndarray of the following form, shape (n_samples, 1)\n",
    "      correct value\n",
    "    batch_size : int\n",
    "      Batch size\n",
    "    seed : int\n",
    "      Seeding random numbers in NumPy\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=None):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1] \n",
    "    \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wo4K2_9YhqXG"
   },
   "source": [
    "#Problem 1 - Creating 2D-Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDOY-_Q_-guZ"
   },
   "outputs": [],
   "source": [
    "# 2d convolutional layer class\n",
    "class SimpleConv2d():\n",
    "    \"\"\"\n",
    "    2-D convolutional layer\n",
    "    Parameters\n",
    "    ----------\n",
    "    initializer : Instances of initialization methods\n",
    "    optimizer : Instances of optimization methods\n",
    "    \"\"\"\n",
    "    def __init__(self, F, C, FH, FW, P, S,\n",
    "                 initializer=None,optimizer=None,activation=None):\n",
    "        self.P = P\n",
    "        self.S = S\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Initialize.\n",
    "        # Use the initializer method to initialize self.W and self.B\n",
    "        self.W = self.initializer.W(F,C,FH,FW)\n",
    "        self.B = self.initializer.B(F)\n",
    "        \n",
    "    def output_shape2d(self,H,W,PH,PW,FH,FW,SH,SW):\n",
    "        OH = (H +2*PH -FH)/SH +1\n",
    "        OW = (W +2*PW -FW)/SW +1\n",
    "        return int(OH),int(OW)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        forward \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of the following form, shape (batch_size, n_nodes1)\n",
    "            Input\n",
    "        Returns\n",
    "        ----------\n",
    "        A : ndarray of the following form, shape (batch_size, n_nodes2)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        N,C,H,W = self.X.shape\n",
    "        F,C,FH,FW = self.W.shape\n",
    "        \n",
    "        OH,OW = self.output_shape2d(H,W,self.P,self.P,FH,FW,self.S,self.S)\n",
    "        \n",
    "        self.params = N,C,H,W,F,FH,FW,OH,OW\n",
    "\n",
    "        A = np.zeros([N,F,OH,OW])\n",
    "\n",
    "        self.X_pad = np.pad(self.X,((0,0),(0,0),(self.P,self.P),(self.P,self.P)))\n",
    "\n",
    "        # Batch\n",
    "        for n in range(N):\n",
    "            # Output channels\n",
    "            for ch in range(F):\n",
    "                # Vertical slide\n",
    "                for row in range(0,H,self.S):\n",
    "                    # Horizontal Slide\n",
    "                    for col in range(0,W,self.S):\n",
    "                        A[n,ch,row,col] = \\\n",
    "                        np.sum(self.X_pad[n,:,row:row+FH,col:col+FW]\n",
    "                               *self.W[ch,:,:,:]) \\\n",
    "                        +self.B[ch]\n",
    "        \n",
    "        return  self.activation.forward(A)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        backward\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray of the following form, shape (batch_size, n_nodes2)\n",
    "            The gradient flowed in from behind.\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : ndarray of the following form, shape (batch_size, n_nodes1)\n",
    "            forward slope\n",
    "        \"\"\"\n",
    "        \n",
    "        dA = self.activation.backward(dZ)\n",
    "        N,C,H,W,F,FH,FW,OH,OW = self.params\n",
    "        \n",
    "        dZ = np.zeros(self.X_pad.shape)\n",
    "        self.dW = np.zeros(self.W.shape)\n",
    "        self.dB = np.zeros(self.B.shape)\n",
    "        \n",
    "        # dZ\n",
    "        # Batch\n",
    "        for n in range(N):\n",
    "            # Output channels\n",
    "            for ch in range(F):\n",
    "                # Vertical slide\n",
    "                for row in range(0,H,self.S):\n",
    "                    # Horizontal Slide\n",
    "                    for col in range(0,W,self.S):\n",
    "                        dZ[n,:,row:row+FH,col:col+FW] += dA[n,ch,row,col]*self.W[ch,:,:,:]\n",
    "                \n",
    "        dl_rows = range(self.P),range(H+self.P,H+2*self.P,1)\n",
    "        dl_cols = range(self.P),range(W+self.P,W+2*self.P,1)\n",
    "\n",
    "        dZ = np.delete(dZ,dl_rows,axis=2)\n",
    "        dZ = np.delete(dZ,dl_cols,axis=3)\n",
    "                \n",
    "        # dW\n",
    "        # Batch\n",
    "        for n in range(N):\n",
    "            # Output channels\n",
    "            for ch in range(F):\n",
    "                # Vertical slide\n",
    "                for row in range(OH):\n",
    "                    # Horizontal Slide\n",
    "                    for col in range(OW):\n",
    "                        self.dW[ch,:,:,:] += dA[n,ch,row,col]*self.X_pad[n,:,row:row+FH,col:col+FW]\n",
    "        \n",
    "        # dB\n",
    "        # Output channels\n",
    "        for ch in range(F):\n",
    "            self.dB[ch] = np.sum(dA[:,ch,:,:])\n",
    "        \n",
    "        # Update\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSg1JUBwMZ6t"
   },
   "source": [
    "#Problem 2/3 - Experiments with 2D Conv. Layers & Output Size after 2DMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2zIlGaWMZAx"
   },
   "outputs": [],
   "source": [
    "def output_shape2d(IH=5,IW=5,PH=0,PW=0,FH=3,FW=3,SH=1,SW=1):\n",
    "    OH = (IH +2*PH -FH)/SH +1\n",
    "    OW = (IW +2*PW -FW)/SW +1\n",
    "    return int(OH),int(OW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8oxdhQ-LN9Ko",
    "outputId": "14343135-6c73-448c-a5f5-e02113d5b1da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n"
     ]
    }
   ],
   "source": [
    "print(output_shape2d(IH=6,IW=6,PH=0,PW=0,FH=3,FW=3,SH=1,SW=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plIV0j2gOd88"
   },
   "source": [
    "2DC Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0SbyOc03Ocj8",
    "outputId": "5372c8c1-bdae-431c-c16a-bd0e736fb6da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.shape: (5, 4, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "N,C,H,W = (5,1,28,28)\n",
    "F,C,FH,FW = (4,1,3,3)\n",
    "\n",
    "S = 1 #Fixed for now\n",
    "P = 1\n",
    "\n",
    "OH,OW = output_shape2d(H,W,P,P,FH,FW,S,S)\n",
    "\n",
    "A = np.zeros([N,F,OH,OW])\n",
    "\n",
    "X_sample = x[0:N].reshape(N,C,H,W)\n",
    "X_pad = np.pad(X_sample,((0,0),(0,0),(P,P),(P,P)))\n",
    "w = np.ones([F,C,FH,FW])\n",
    "B = np.ones(F)\n",
    "\n",
    "# Forward\n",
    "\n",
    "# Batch\n",
    "for n in range(N):\n",
    "    for ch in range(F):\n",
    "        for row in range(0,H,S):\n",
    "            for col in range(0,W,S):\n",
    "                A[n,ch,row,col] = \\\n",
    "                np.sum(X_pad[n,:,row:row+FH,col:col+FW]*w[ch,:,:,:]) +B[ch]\n",
    "                \n",
    "print('A.shape:',A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vWs05eZrR7cJ",
    "outputId": "54d92904-765d-4544-d909-38f2360f2cbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dZ.shape: (5, 1, 28, 28)\n",
      "dW.shape: (4, 1, 3, 3)\n",
      "dB.shape: (4,)\n"
     ]
    }
   ],
   "source": [
    "#Backward\n",
    "dA = np.ones(A.shape)\n",
    "\n",
    "dZ = np.zeros(X_pad.shape)\n",
    "dW = np.zeros(w.shape)\n",
    "dB = np.zeros(B.shape)\n",
    "\n",
    "for n in range(N):\n",
    "    for ch in range(F):\n",
    "        for row in range(0,H,S):\n",
    "            for col in range(0,W,S):\n",
    "                dZ[n,:,row:row+FH,col:col+FW] +=dA[n,ch,row,col]*w[ch,:,:,:]\n",
    "\n",
    "dl_rows = range(P),range(H+P,H+2*P,1)\n",
    "dl_cols = range(P),range(W+P,W+2*P,1)\n",
    "\n",
    "dZ = np.delete(dZ,dl_rows,axis=2)\n",
    "dZ = np.delete(dZ,dl_cols,axis=3)\n",
    "\n",
    "for n in range(N):\n",
    "    for ch in range(F):\n",
    "        for row in range(OH):\n",
    "            for col in range(OW):\n",
    "             dW[ch,:,:,:] +=dA[n,ch,row,col]*X_pad[n,:,row:row+FH,col:col+FW]\n",
    "\n",
    "for ch in range(F):\n",
    "    dB[ch] = np.sum(dA[:,ch,:,:])\n",
    "\n",
    "print('dZ.shape:',dZ.shape)\n",
    "print('dW.shape:',dW.shape)\n",
    "print('dB.shape:',dB.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npg6oh4tVwm1"
   },
   "source": [
    "Problem 4 - Creating a Maximum Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-ph7YZwVv4s"
   },
   "outputs": [],
   "source": [
    "class MaxPool2D():\n",
    "    \n",
    "    def __init__(self,P):\n",
    "        self.P = P\n",
    "        self.PA = None\n",
    "        self.Pindex = None\n",
    "        \n",
    "    def forward(self,A):\n",
    "        N,F,OH,OW = A.shape\n",
    "        PS = self.P\n",
    "        PH,PW = int(OH/PS),int(OW/PS)\n",
    "        \n",
    "        self.params = N,F,OH,OW,PS,PH,PW\n",
    "        \n",
    "        # Pooling filter\n",
    "        self.PA = np.zeros([N,F,PH,PW])\n",
    "        self.Pindex = np.zeros([N,F,PH,PW])\n",
    "        \n",
    "        for n in range(N):\n",
    "            # Output channels\n",
    "            for ch in range(F):\n",
    "                # Vertical slide\n",
    "                for row in range(PH):\n",
    "                    # Horizontal Slide\n",
    "                    for col in range(PW):\n",
    "                        self.PA[n,ch,row,col] = \\\n",
    "                        np.max(A[n,ch,row*PS:row*PS+PS,col*PS:col*PS+PS])\n",
    "                        \n",
    "                        self.Pindex[n,ch,row,col] = \\\n",
    "                        np.argmax(A[n,ch,row*PS:row*PS+PS,col*PS:col*PS+PS])\n",
    "                        \n",
    "        return self.PA\n",
    "    \n",
    "    def backward(self,dA):\n",
    "        \n",
    "        N,F,OH,OW,PS,PH,PW = self.params\n",
    "        dP = np.zeros([N,F,OH,OW])\n",
    "        \n",
    "        for n in range(N):\n",
    "            # Output channels\n",
    "            for ch in range(F):\n",
    "                # Vertical slide\n",
    "                for row in range(PH):\n",
    "                    # Horizontal Slide\n",
    "                    for col in range(PW):\n",
    "                        idx = self.Pindex[n,ch,row,col]\n",
    "                        tmp = np.zeros((PS*PS))\n",
    "                        for i in range(PS*PS):\n",
    "                            if i == idx:\n",
    "                                tmp[i] = dA[n,ch,row,col]\n",
    "                            else:\n",
    "                                tmp[i] = 0\n",
    "                        dP[n,ch,row*PS:row*PS+PS,col*PS:col*PS+PS] = tmp.reshape(PS,PS)\n",
    "        \n",
    "        return dP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3JsCKwuKdQ63",
    "outputId": "ac411bf0-77f3-4064-d90b-26124c1b7d4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1 8 0 3 1 8]\n",
      "   [6 5 0 0 3 2]\n",
      "   [2 4 0 6 2 4]\n",
      "   [6 2 6 3 2 4]\n",
      "   [2 1 1 0 7 5]\n",
      "   [6 0 0 4 3 1]]]]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randint(0,9,36).reshape(1,1,6,6)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IpQ_KyFpeBzk",
    "outputId": "5ccaa8f0-8077-4b7b-b7c7-ac6064ccc3d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 3, 3)\n",
      "[[[[8. 3. 8.]\n",
      "   [6. 6. 4.]\n",
      "   [6. 4. 7.]]]]\n"
     ]
    }
   ],
   "source": [
    "Pooling = MaxPool2D(P=2)\n",
    "A = Pooling.forward(X)\n",
    "\n",
    "print(A.shape)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bDaJp9_TfE9p",
    "outputId": "4cf8590f-69e5-4e92-e5cd-a3ff8430fc93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[1., 1., 1.],\n",
       "         [2., 1., 1.],\n",
       "         [2., 3., 0.]]]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pooling.Pindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mwy0h-_QfReh",
    "outputId": "4cb3dcb3-d1f5-40f6-bc3d-cf0ac6fb2d81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1 5 1]\n",
      "   [8 1 5]\n",
      "   [5 4 5]]]]\n"
     ]
    }
   ],
   "source": [
    "dA = np.random.randint(0,9,9).reshape(A.shape)\n",
    "print(dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GgMsH2ucf0pZ",
    "outputId": "0d9541cc-3a86-4d42-cdab-ba46a1482c2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0. 1. 0. 5. 0. 1.]\n",
      "   [0. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 1. 0. 5.]\n",
      "   [8. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 5. 0.]\n",
      "   [5. 0. 0. 4. 0. 0.]]]]\n"
     ]
    }
   ],
   "source": [
    "dZ = Pooling.backward(dA)\n",
    "print(dZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lbRmFgpgNPq"
   },
   "source": [
    "Problem 5 - Creating An Average Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRE7bfubgMR5"
   },
   "outputs": [],
   "source": [
    "class AveragePool2D():\n",
    "    \n",
    "    def __init__(self,P):\n",
    "        self.P = P\n",
    "        self.PA = None\n",
    "        self.Pindex = None\n",
    "        \n",
    "    def forward(self,A):\n",
    "        N,F,OH,OW = A.shape\n",
    "        PS = self.P\n",
    "        PH,PW = int(OH/PS),int(OW/PS)\n",
    "        \n",
    "        self.params = N,F,OH,OW,PS,PH,PW\n",
    "        \n",
    "        # Pooling filter\n",
    "        self.PA = np.zeros([N,F,PH,PW])\n",
    "        \n",
    "        for n in range(N):\n",
    "            # Output channels\n",
    "            for ch in range(F):\n",
    "                # Vertical slide\n",
    "                for row in range(PH):\n",
    "                    # Horizontal Slide\n",
    "                    for col in range(PW):\n",
    "                        self.PA[n,ch,row,col] = \\\n",
    "                        np.mean(A[n,ch,row*PS:row*PS+PS,col*PS:col*PS+PS])\n",
    "                        \n",
    "        return self.PA\n",
    "    \n",
    "    def backward(self,dA):\n",
    "        \n",
    "        N,F,OH,OW,PS,PH,PW = self.params\n",
    "        dP = np.zeros([N,F,OH,OW])\n",
    "        \n",
    "        for n in range(N):\n",
    "            # Output channels\n",
    "            for ch in range(F):\n",
    "                # Vertical slide\n",
    "                for row in range(PH):\n",
    "                    # Horizontal Slide\n",
    "                    for col in range(PW):\n",
    "                        tmp = np.zeros((PS*PS))\n",
    "                        for i in range(PS*PS):\n",
    "                            tmp[i] = dA[n,ch,row,col]/(PS*PS)\n",
    "\n",
    "                        dP[n,ch,row*PS:row*PS+PS,col*PS:col*PS+PS] = tmp.reshape(PS,PS)\n",
    "        \n",
    "        return dP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WM9jcpp5jnBV",
    "outputId": "2e297958-997d-4db8-f19c-21e883ce6245"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[6 1 4 0 0 3]\n",
      "   [7 2 1 1 1 6]\n",
      "   [0 4 5 7 4 4]\n",
      "   [4 7 1 5 1 5]\n",
      "   [5 5 1 0 4 4]\n",
      "   [3 0 7 5 8 6]]]]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randint(0,9,36).reshape(1,1,6,6)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kw6Z_5y-tARr",
    "outputId": "39fec28c-b80b-4193-fffb-ab31216e8018"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 3, 3)\n",
      "[[[[4.   1.5  2.5 ]\n",
      "   [3.75 4.5  3.5 ]\n",
      "   [3.25 3.25 5.5 ]]]]\n"
     ]
    }
   ],
   "source": [
    "Pooling = AveragePool2D(P=2)\n",
    "A = Pooling.forward(X)\n",
    "\n",
    "print(A.shape)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x087QvQdtVZQ",
    "outputId": "0fa49920-75c8-4d75-983d-b3234e4f3f49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[5 8 6]\n",
      "   [6 3 2]\n",
      "   [8 1 4]]]]\n"
     ]
    }
   ],
   "source": [
    "dA = np.random.randint(0,9,9).reshape(A.shape)\n",
    "print(dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f9fkqyH3tyZO",
    "outputId": "4e8837d3-019a-4c56-c66e-5a124bd53429"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1.25 1.25 2.   2.   1.5  1.5 ]\n",
      "   [1.25 1.25 2.   2.   1.5  1.5 ]\n",
      "   [1.5  1.5  0.75 0.75 0.5  0.5 ]\n",
      "   [1.5  1.5  0.75 0.75 0.5  0.5 ]\n",
      "   [2.   2.   0.25 0.25 1.   1.  ]\n",
      "   [2.   2.   0.25 0.25 1.   1.  ]]]]\n"
     ]
    }
   ],
   "source": [
    "dZ = Pooling.backward(dA)\n",
    "print(dZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJhuBb5yukfc"
   },
   "source": [
    "#Problem 6 - Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8uKW0ekpujuD"
   },
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    def __init__(self,):\n",
    "        pass\n",
    "    def forward(self,X):\n",
    "        self.shape = X.shape\n",
    "        return X.reshape(len(X),-1)\n",
    "\n",
    "    def backward(self,X):\n",
    "        return X.reshape(self.shape)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cOU4i5c6wzB1",
    "outputId": "676680ed-e059-4058-e7cb-e22f8304c6ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward_shape Display: (20, 50)\n",
      "Backward_shape Display: (20, 2, 5, 5)\n"
     ]
    }
   ],
   "source": [
    "TEST = np.zeros([20,2,5,5])\n",
    "flt = Flatten()\n",
    "flat_forward = flt.forward(TEST)\n",
    "print('Forward_shape Display:', flat_forward.shape)\n",
    "print('Backward_shape Display:',flt.backward(flat_forward).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rFz-DrIyE4Z"
   },
   "source": [
    "#Problem 7 - Training and Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BLTM2L-zyDza"
   },
   "outputs": [],
   "source": [
    "# Scratch CNN\n",
    "class Scratch2dCNNClassifier():\n",
    "    \"\"\"\n",
    "    N-Layer Convolutional Neural Network Classifier\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    self.n_epoch : epoch number\n",
    "    self.n_batch : Number of batches\n",
    "    self.verbose : Visualizing the learning process\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, NN, CNN, n_epoch=5, n_batch=1, verbose = False):\n",
    "        # Parameters\n",
    "        self.n_epoch = n_epoch\n",
    "        self.n_batch = n_batch\n",
    "        self.verbose = verbose\n",
    "        self.log_loss = np.zeros(self.n_epoch)\n",
    "        self.log_acc = np.zeros(self.n_epoch)\n",
    "        self.NN = NN\n",
    "        self.CNN = CNN\n",
    "        \n",
    "    def loss_function(self,y,yt):\n",
    "        delta = 1e-7\n",
    "        return -np.mean(yt*np.log(y+delta))\n",
    "    \n",
    "    def accuracy(self,Z,Y):\n",
    "        return accuracy_score(Y,Z)\n",
    "                \n",
    "    def fit(self, X, y, X_val=False, y_val=False):\n",
    "        \"\"\"\n",
    "        Train a neural network classifier.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of the following form, shape (n_samples, n_features)\n",
    "            Features of training data\n",
    "        y : ndarray of the following form, shape (n_samples, )\n",
    "            Correct answer value of training data\n",
    "        X_val : ndarray of the following form, shape (n_samples, n_features)\n",
    "            Features of validation data\n",
    "        y_val : ndarray of the following form, shape (n_samples, )\n",
    "            Correct value of validation data\n",
    "        \"\"\"\n",
    "        for epoch in range(self.n_epoch):\n",
    "            # Mini-batch processing\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.n_batch)\n",
    "            \n",
    "            self.loss = 0\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                                \n",
    "                # Forward propagation\n",
    "                forward_data = mini_X_train[:,np.newaxis,:,:]\n",
    "                \n",
    "                # Conv\n",
    "                for layer in range(len(self.CNN)):\n",
    "                    forward_data = self.CNN[layer].forward(forward_data)\n",
    "                \n",
    "                # Flatten\n",
    "                flt = Flatten()\n",
    "                forward_data = flt.forward(forward_data)\n",
    "                \n",
    "                # NN\n",
    "                for layer in range(len(self.NN)):\n",
    "                    forward_data = self.NN[layer].forward(forward_data)\n",
    "                    \n",
    "                # Predicted value\n",
    "                Z = forward_data\n",
    "                \n",
    "                # Back propagation\n",
    "                backward_data = (Z - mini_y_train)/self.n_batch\n",
    "                for layer in range(len(self.NN)-1,-1,-1):\n",
    "                    backward_data = self.NN[layer].backward(backward_data)\n",
    "                   \n",
    "                    \n",
    "                backward_data = flt.backward(backward_data)\n",
    "                \n",
    "                for layer in range(len(self.CNN)-1,-1,-1):\n",
    "                    backward_data = self.CNN[layer].backward(backward_data)\n",
    "                \n",
    "                # Loss function\n",
    "                self.loss += self.loss_function(Z,mini_y_train)\n",
    "                \n",
    "            self.log_loss[epoch] = self.loss/len(get_mini_batch)\n",
    "            self.log_acc[epoch] = self.accuracy(self.predict(X),np.argmax(y,axis=1))\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Estimate using a neural network classifier.ã€‚\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of the following form, shape (n_samples, n_features)\n",
    "            Sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            ndarray of the following, shape (n_samples, 1)\n",
    "            Estimation results\n",
    "        \"\"\"\n",
    "        pred_data = X[:,np.newaxis,:,:]\n",
    "        \n",
    "        # Conv\n",
    "        for layer in range(len(self.CNN)):\n",
    "            pred_data = self.CNN[layer].forward(pred_data)\n",
    "                \n",
    "        pred_data = flt.forward(pred_data)\n",
    "        \n",
    "        # NN\n",
    "        for layer in range(len(self.NN)):\n",
    "            pred_data = self.NN[layer].forward(pred_data)\n",
    "            \n",
    "        return np.argmax(pred_data,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GBND4WLh8LqU"
   },
   "outputs": [],
   "source": [
    "NN = {0:FC(7840, 400, HeInitializer(), AdaGrad(0.01), ReLU()),\n",
    "      1:FC(400, 200, HeInitializer(), AdaGrad(0.01), ReLU()),\n",
    "      2:FC(200, 10, SimpleInitializer(0.01), AdaGrad(0.01), Softmax()),\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnL-5VFP_PvA"
   },
   "outputs": [],
   "source": [
    "CNN = {0:SimpleConv2d(F=10, C=1, FH=3, FW=3, P=1, S=1,\n",
    "                      initializer=SimpleInitializerConv2d(),\n",
    "                      optimizer=SGD(),\n",
    "                      activation=ReLU())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VyARBCAbAPOB",
    "outputId": "293d4c4e-a255-483a-a463-35d46a3f6e13"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-110-be33727ef1fa>:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
      "<ipython-input-110-be33727ef1fa>:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
      "<ipython-input-110-be33727ef1fa>:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
      "<ipython-input-110-be33727ef1fa>:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
      "<ipython-input-110-be33727ef1fa>:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
      "<ipython-input-110-be33727ef1fa>:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
      "<ipython-input-110-be33727ef1fa>:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
      "<ipython-input-110-be33727ef1fa>:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
      "<ipython-input-110-be33727ef1fa>:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
      "<ipython-input-110-be33727ef1fa>:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "cnn1 = Scratch2dCNNClassifier(NN=NN,CNN=CNN,n_epoch=10,n_batch=200,verbose=False)\n",
    "\n",
    "cnn1.fit(x_train[0:1000],y_train[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "medHx_5JAt1S",
    "outputId": "478ebcc7-72a5-4521-d4e8-b4a994795785"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.800\n"
     ]
    }
   ],
   "source": [
    "y_pred = cnn1.predict(x_valid[0:100])\n",
    "\n",
    "accuracy = accuracy_score(np.argmax(y_valid[0:100],axis=1), y_pred)\n",
    "print('accuracy:{:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMRRURTNB3GQ"
   },
   "source": [
    "Visualizing the Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "F1E1yQ4WB1jA",
    "outputId": "b743e796-0029-4ff1-da82-15f048d6980e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA74AAAGHCAYAAACXlI5NAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAABkkUlEQVR4nO3dd5hU5dnH8e9NkS4oUlQEotJMVKKrYEOKYu+a+IpGsWDBKJYYI1YMdhElNkyUqFixRqOigD0WNMauWAAbiAoKLCKw9/vHc8Ydh53dmd3ZPVN+n+s619k5be4Zln3OfZ5m7o6IiIiIiIhIsWoUdwAiIiIiIiIi9UmJr4iIiIiIiBQ1Jb4iIiIiIiJS1JT4ioiIiIiISFFT4isiIiIiIiJFTYmviIiIiIiIFDUlviIiIiIiIlLUlPiKpGFmk8zMzezpWp6/s5ndYmazzGyxmS0xs4+i6w7N8Bq9zOwaM3srusZyM/vMzF4xsxvM7GAzWzvNuS3N7CQzm2FmC8zsJzP7xszeNrMHzGyUmf26Fp/r6eh7SV4qzGxRFNe5VcVkZgOTju9ei/ftYmYXRu+xIPouvjSzp8zsZDNrVcU53auINdNlUrYxioiUCjPbN+nv5ZNZntvSzI43s3+Z2VwzKzezpWb2qZlNMbNDzaxFDddYPypvnjOzr6Iy7vuojPu7me1kZpZyzhGJmDOIMe2xZnZ+mnJjaVTm/9PMts7i+xifdI3RmZ4XnZvx9xC9djObY2ZtarjuBmb2QzYxpZTzycuP0b/zvWa2U5pzE/cWk7L5/NG5ZmYHmNld0e9QefQdvGdm15tZ/zTnTUoTbyZL92zjlDzg7lq0aKliASYBDjyd5XlrA49F5yaWpcCSlG2PA2tXc50RwPKk4yuA74AfU64zqopzNwY+SjluCfBDyrY3avG9PB2duwyYFy0LUq77OdAr5byBSfu7Z/mepwHlSeevjL6LiqRtXwI7pZy3QVKMqcuqpO+lqv1Xx/07qEWLFi35ugAPJP39XQWsn+F5ewFfZVA+fQEMruJ8A86OyqDk4xdWUT6+AnRJOveIxL4M4kx7LHB+0udOLjdWpHwnozJ4n6bA10nnfZDh95j19wC0jcpnB66r4fqPRse9DjTJMKbkcn5B0veSGuMVVZz7dLRvUpa/h92iz5d8/R+qeM9bgeYp516dpvz/Pum8dPcQG8T9f1BL9otqfEVyyMzaAc8DuxKS1r8Cv3L3Vu7emvAH+gJCobQL8Hx0Tup1tgNuANYAngJ2JPzBXhtoAfQETgT+Q/jDnHxuE+BBYCPCH+fjCQl2a3dfE2gP7E0oBJbV4ePe7e6do6UDoUA9Jfrc6wN3pT5trw0zuwi4gvC5nyR8F82i76IVcCDwIbAu8G8z2zNxrrt/lhTjLxbgs+iwK9Icc3JdYxcRKUZmtg6wB+Gh7h2EFoSHZXDeEYTyqTPwQXTOOknlUzvC3/SngfWAAVVc5u/AhUBzYCqhLG3p7mu5e3OgK3AC8DGwFeFBcH35LKVcaQ5sB7xB+E6uNLPf1HCN3YAOwDOEsqxnuhrKFFl/D+7+PXBcdP5xZrZjVRc2s0OB3QkPmY9095UZxJNqq6TvpRXQF3gh2neame1bi2umxtmdcB+0FSHZPR3o7O5runsLoDcwgfCQ/DDgcTNrmjjf3U9Oc39wctIxVd5DuPtnSOGJO/PWoiVfF2pR4wvcG51TDuxYzXEDqKzBvKeK/XdF+/4HNK7hPVOfYO5K5ZPKsmzOzfAzPk01T2WBc5Lef5uk7QOTtnfP8L32SDrnb9Uc14bKhwDfkUHNAzA7Ov78uH/XtGjRoqWQFuCP0d/PO6LyzIH3ajhncyprIh8FWtRw/O+B01K2HZtUJpxbw/lNgIuAAUnbjkicn8FnTHsslTW+s9Oc2w34KTrmkhre577ouBHAudHP19dwTl2/h9ujc2el/jsAHYFvov0XZvl7UW05D6xFZQuxJ1L2VXtvUcW1GieV+18Dm1Rz7O+obOV1WV3+7bUU9qIaX5EcMbMywpNqCAXRM+mOdfdngfOilweZ2ZYph2warR9z91XVva+7/5jm3PnuPjPLc3PhzqSfUz9Xti6N1v8DRqU7yN0XAwcTHiasBZxZx/cVEZH0Do/Wk4HngLlA7xr6tf4VaEZownyIu1fb4sjd7wbGJV6bWXNgTPTyEXcfU+WJleevdPezovgalLvPIdTeAmyS7jgL42HsSUiS7yU8SAD4vZk1S3NOLr6Hk4H5hNrwv6acMoHQMuwdQo1yzrj7QuCJ6GVd7w/2BxI14ye4+7vVvO89hFZ0ACeb2bp1fG8pUEp8RXLn2Gi9CLg2g+OvJfQjST431fp1iGftqIBsaF8k/bxmbS8SNfdODL51qdfQ1Cq60ZgcvRxuZmvU9r1FRKRqFgZF3BL4Fpjq7k7lA8/D05yzPqEFD8A1Hprc1ii6dsL+hNpIyCIhS7lGQ0p09WlczTH/R+jS9Li7L3T3j4CXCQ9w905zTp2/B3f/ltBdCmCUmfUDMLN9qKwdPdLdf8r0+llI3CPU+v4gMiJaf+DuUzI4/hJCk+c1gOF1fG8pUEp8RXJnYLSeWtOTbAB3Lyf0y0k+NyFRU/t7M9s/yzgS5zYFbqhp5MZ60DXp50V1uM7AaF0B/CvDcx6M1q2Asjq8t4iIVC2R3N7j7iuinxMPHQ9O89BxIJWJ4MO1fN9B0Xq+u79Sy2s0iKjvaY/o5SfVHJpcc07Kz1U+RCBH30OULN5HyAX+YWYdgeui3VfV43ecuEdYVNsLRP10t41ePpTJOR765L4WvRxY2/eWwqbEVyQHoj/CiQE0/pfFqW9G6x7RoFQJlxGa7TYF7jOz2RamRjrezLY0s+qeID9NGCQDQsH5lZk9YmbnmNmuVQ2mlWPHJP38ch2uk2ge9rG7L8nwnDeTfu5Th/cWEZEUUdlzaPQy0SwXd38LeIswq8FeVZya+Hu8nDCoVW0krpFNGdugzKyxmW1DGPE6MYjS7WmO7UMYlGkxv3y4ezehxnUXM+tUxam5/B5GEsbF+DVh9Ob1CP1+z83BtVcTNTFO1PzX5f6gO9Ay+rk291y6PyhRSnxFciN53tpvszjvm6qu4e7vADsR+thAGCjjCMLT2JnAtxbm8d0g9YJRc6bEqM0VhNrPPQh9gh6Lzp1hZrtnEWe1osK+h5ldTGVf3P+4+2vVnFaTxPdR2++zfR3eW0REVrczYQT9OVSO0JtQXU1l4u/xwjo0PU5c47tanl8fNjCzeYmFMFPCi4QRjCEMnpguwUt8Tw8ktxJz968JMxg0AYZVcV7Ovgd3nw+cFL1cnzCg05GZtFrLhpmtZWZDCZ8r0Qrtb3W4ZF3vuXR/UKKU+IrkKXf/D2GgqoGEQZ6eJQzXD2HqoGOBt8xshyrO/cHdDwd+RZhi6H7CjQqE//cDgUfN7Mo6hHh4YiJ3wpQHHxIGlWpMeKJ/cB2uLSIi+eeIaH1nFQnsnYTEaTcz69CgUcWnEdApaUnU8v4I7OHuF1R1UlRznpj+6Y4qDqmpuXPOuPtk4L/Rywfd/fkcXfrTpHuE7wiDWv2a8ED+LHd/otqzReqBEl+R3Eh+8prNk8R10lwDCLW37v6Mu5/p7jsSnnJuD/yTcIPRFrjbzFpUdXF3n+vu4939AHfvTqg5PjXpvU6NBrOojR8Jo0LOB74iJL6PEuYN3MLd59byugmJGHP2fYqISO2YWVsgUV6slqxFf/OfI9RUHpKyO1Ert1Yd5ndPXGPtao9qWHPc3dzdCIMm9QauJ8yte2PU17cqOxGaFX8NPFXF/gcJ3Z02M7PNU/bVx/fwQ8o6F76h8h5hLqG12rWEaRYvruO163rPpfuDEqXEVyQHogE+Po5ephZS1dksWs+qadTi6H1WufsL7n4ElX1w1iXM3ZtJnHPd/SpgG0KhCnBkFvEmu9srJ3Jfz917ufue7n59NHBXXb0XrTcys9YZnrNZ0s9ppzYQEZGs/Z6Q0AG8majNS14Ic/rC6jWVib/nzYBetXz/xDWyKWNT/TyFX7oHxkkSfUgzmvbP3Ve4+wfufgJwE9AFuNPMqrrXTnw/HYGVVXyPi5PeP913WZfvoSFslXSP0M3dt3L3E939vzWfWqPZhGblULt7Lt0flCglviK5MyNaD82gQMXMWgJDo5dp5/ytxj+Sfu6ZzYnu/iGQaM6U1bkNKPF9NqLqwVKqsm+0Xkrl6NYiIlJ32TS7/a2ZbZr0+hlCKyVIP01PTRJlQqca5guuTnJ/0Jrmck3sz6YPacKfCdMV9qeySTMAZrYmlWVVJoalDH6Zi++hoEWVDYk+5hm1WovGREnMHVybey4pAkp8RXJnYrRuRxgpsSYjCU2VAW6sxfstTfq5NnPtJc6vj3n66szdX6BycK8zUgr+1ZhZNyoHAplUT/MPioiUHDPrQeX0MX0J88ymWxIjFP+cKLv758C/o5d/jJK/TN43uVn0A8CC6Oezs4g9+RrJtY3bph6bIrE/6xpKd19IaNYLcH5K+fU7oAXwGdV/j+0JSXdHftmqKxffQzFI3HP1MrMDMzj+TELeswK4pd6ikrymxFckR9z9VcIgUgBjzGxAumOjAakSg17c5+4zU/YPrGHKIvhlH6o3ks79jZl1ru7EaIqEwann5qG/ROu+wPh0B0VNoe8iNA1bRBgMTEREcuMP0fp/7v4/d1+UbgHujY4dllKOnU2YzqgLcIeZNacaZvY7wpgUAEQjDZ8XvdzLzM6p4fwmZnYR8PMAkO7+DWGgSICT05WzZrYFlXO93l/VMRmYQPi83amcAgoqHwjcX9336O7fUTlHbfJDhDp/D0XifuDV6OfrzGyTdAdGv0vHRS+vcfcv6zs4yU9KfEVq1tTM1qlhSYzkeDRhROMWwFQzG2NmicnaMbMNzOw8YGp0zAf8ct7bhCuAj8zsfDPbKnF9M2tkZr+Kpg26Jjr2DSoLcgiF9admdpuZ7WVmPw+AYWZrmtkwQjPntoTRFesypUBtrVXD99kawN3/BVwenTPSzJ4wsx0SfabMrIWZHUBo1tyf8CT3iGiiehERqaOopjDRXDeTJPBfhL/FnYFdEhvd/Q1CSycnTLH3XzM7NKWMamtm+5vZDMJ8tm2SL+zu1xOm6oPwgPlxM9s5OYk2sy5mdhyhL+xfWP1e92zCPLllhNkNfpuoDTWzlmZ2MGHqv0bA21Q96nKN3H0ecFv08i9R+b0RYYBKyOy7TByzl5mtlXTtXHwP+apZBvdcjdx9FWH2iPlAB+BFMzvVzDomLmRmPc3sasK/YSPCvc9ZcXwoyRPurkWLlioWYBKhgM5kGZh0XntCYpu8fwlhsIrkbU8C7dO8939Sjl1FGIXwp5Tt7wLdUs49tor4FhNGa0zetgw4vBbfy9PR+ZOyPG9gFt/npJRzz4jiTexfEX0fFUnbvgKGZhHP7Oi88+P+XdOiRYuWfF2AQUl/Z3+d4TmPR8ffXcW+fQnJSk1l1GxgQBXnG6HG88ekYyuiMmFZyjWeB9ar4hpHEGpjk8vDbwhT8yW2vQdsmObznZ+IsYbvoVdUfjvwf4SWXg7MAxpl8D2uQegr7MDxuf4ekq5Vq3K9iuskl/Pdszz36ZSYq1u6J533K8LD7+T93xMG8EzeNhlokWEsRyTOi/v/n5bcLoXy9EekYLj7t+4+lNAn51bgE0IB1Sj6+TZgN3ff2d3TDZoxiHBzMAF4iVCQtSEUoJ8BjwBHAX3dfU7yie5+I7AFMJrw1HoOYW7BFoT+Qi8BFwF93P2fufnU9cvdLyPcQFwEvEa4QWpNmApiOmGu4h7uPjW2IEVEilOime2H7v5OtUdWui9a721m7ZJ3uPuDwIaE2t9/A58TpkBqQkh2pxC68vRy9+TWTInz3cP8uBsTEskXCX1eWxMeir5DGFV5kLtv71U0a3X3ScAmwNXAW9F5bQll7TTgRMK0fJ9k+Hmr5O4fAA9HL8+issn4Q+5ekcH5PxHKe0gZXCwX30MxcPdPga0IfafvJUydtAbhfulDQl/gbd19mIdm4lLCzMOTDREREREREZGipBpfERERERERKWpKfEVERERERKSoKfEVERERERGRoqbEV0RERERERIqaEl8REREREREpak3iDqAhrbPOOt69e/e4wxARkSLx2muvfePuHeKOo5CpbBYRkVxKVzaXVOLbvXt3Zs6cGXcYIiJSJMxsTs1HSXVUNouISC6lK5vV1FlERERERESKmhJfERERERERKWpKfEVERERERKSoKfEVERERERGRoqbEV0RERERERIqaEl8REREREREpakp8RUREREREpKgp8RUREREREZGipsRXREREREREipoS3yIzeTJ07w6NGoX15MlxRyQiIiIiIpKic2cwW33p3Lle3q5JvVxVYjF5MowYAeXl4fWcOeE1wLBh8cUlIiIiIiLyC/PnZ7e9jlTjW0RGj65MehPKy8N2ERERERGRUqXEt4jMnZvddhERERERkVKgxLeIdO2a3XYREREREZEG9fHHcPzxDf62SnyLyNixYVCrVKNGNXgoIiIiIiIivzR5MvTsCTff3OBvrcS3iFRUhGXttcOAaOutBy1awC23wJIlcUcnIiIiIiIlxR2eeQZeeSW8HjQITjsNPv0UOnWq+px02+tIiW+RWLgw/A717w8LFoQE+Isv4MEH4e234bDDwjYREREREZF6VVEREpFttoGBA+GSS8L29daDyy4L63nzQmKcusybVy8hKfEtEpddBt99Bzfc8MvmzkOHwrhxIfn9+uv44hMRERERkRJw333w61/DfvuFGrnrrgtNnGOmeXyLxNlnw/bbw+abr77vpJPgqKOgdeuGj0tERERERIrckiXQrBk0bQqzZ0Pz5nDnnXDggdAkP1JO1fgWuJUr4ccfoVUr2GOPqo8xC0nv8uVw3HHw2msNG6OIiIiIiBShBQvg3HPDNDJ33BG2nXQSvP46HHxw3iS9oMS34E2YAJtuGn7navLDD/D447DPPvDVV/Ufm4iIiIiIFKHZs+GPf4Ru3eDCC2HHHUNSAqHW1yzW8KqixLeAff55eMDSowess07Nx3foAA89BIsWwb77hppiERERERGRrBxwANx4Y6jVffddeOAB2GKLuKOqlhLfAnbKKaGp89/+lvlDlc03h9tuCyOKH3NMGDhNREREREQkreefh4MOgu+/D68nToRPPgnz8fbpE29sGVLiW6AeewymTAmDWm24YXbn7rdfaJHw8MPh91VEREREROQXKirgkUfCCLo77ABPPx2migHYckvo0iXW8LKlxLdA3Xor9O4Np59eu/NHj4a33oKNNsptXCIiIiIiUuAWLw5NRffaK/SvnDAB5syB7baLO7JaU+JboCZPhqeeCqOG14ZZGHzNHa65Bt55J7fxiYiIiIhIAVm6FKZODT+3aRMGrLrtNpg1C048EVq2jDe+OlLiW2DmzIGvv4ZGjWD99et+vYUL4eKLYe+94dtv6349EREREREpIN9+C2PGhBGa99ijcvqXv/0NDj00jNJcBJT4FhB3OOII2GabMKhVLqy9dhiE7YsvQn/1FStyc10REREREclj8+eH0XK7doXzzoNttw39eNddN+7I6kVWia+ZdTGzm83sSzNbbmazzWy8ma2V4fmtzGyYmd1hZu+b2VIzW2xmM83sNDNbI815Xs3yUjafoZBNnhx+F884I7dzQffvDzfdBDNmwMkn5+66IiIiIiKSZxI1XT/+GKYkOuCAMPjPww8XdB/emmScPpnZRsCLQEfgIeB9YGvgZGBXM9vO3WtqLLsDcDvwHTADeBBYC9gbuALY38yGuHtVM8zOASZVsf3zTD9DIVu4EE49NSSpxxyT++sfdlgYpO3yy2HECOjbN/fvISIi8TOzLsAYYFegPfAVoTy+wN0XZnGd7YE/AZsDnYGvgbeBa9z98RyHLSIi2ejcOdTopmrWDAYNClPEdOsWmn2ulVEdZsHLpt7wOkLSe5K7T0hsNLNxwCnAWOC4Gq4xDzgUuNfdf0q6xunA08C2wEjgyirOne3u52cRb1E566zQ/H7q1NC/tz5cdFGY6khJr4hIccrRQ2zM7HjCfcFS4AHCQ+guwP7AbmZ2truPrZ9PISIiNaoq6QVYvhz69QtTFTVqVDJJL4C5e80HhYLyI2A2sJG7VyTta0N4WmxAR3dfWqtAzA4BJgOPuPteKfsceMbdB9bm2gllZWU+c+bMulwiFhUVoV95584wblzDvOfUqWGqI013JCKSnpm95u5lcceRKTN7AhhK+ofYN7p7tQ+xzawpsABoBvR19w+S9vUB/gtUAGu5+/KaYirUsllEpEFVVMD338M334TasI02gg4d4MMP4ZZbwrbEvm++gXffTX+tDPK/QpaubM60xndQtJ6anPQCuPtiM3uBUJD2B6bVMsbEsErphm1qZ2ZHEppTfQ+85u4l0b+3USO44w5Ytaph3m/p0pBor7MOvPQSrLlmw7yviIjUn+gh9lDCQ+xrU3afB4wADjOz02p4iL020BZ4MznpBXD398zsQ2BToDVQY+IrIlJw0jUj7tQJ5s2r+fyKijBS7RprwJIlMH366onrsGGhSfJ//wu77BK2VySlYXfeCQcfDF9+CVdcEW7c27cP6969q098S1SmiW+vaP1hmv2zCIVpT2qf+B4ZrdP1C9oc+EfyBjP7H3CYu79Vy/fMe/fdB336wCabQOPGDfOerVrBXXfB0KFwyCHw0EMN994iIlJvcvUQ+2tCjW9PM+vh7rMSO8ysJ9ADeCOTJtMiIgUpXTPi+fPhwQdDktqrF2y/PSxeHGqUvvmmMrH97jsYOxb+8pfwep99Kq+xxhohgd1++/C6U6cw+FQiqU2sf/vbsH/AAPjpJzD7ZSypryXjxLdttP4+zf7E9na1CcLMTiQMsvEGcHMVh4wD7iMk3j8CvYE/AwcC082sr7t/kebaIwhPsenatWttwovN55+H6YsGDw7JZ0MaPBgmTIATTgj9iy+9tGHfX0REci4nD7Hd3c1sJGGwytfM7AHgS2B9YD/gHeDgXAUtIlJQ9tsvrE88MSSvzZvDnDkhYe3btzJxTSS2660Hr75amdS2bv3LpHW99eD669O/X30N/lOEcjgpTu2Y2f7AeMLAVwe4+2ozybr7aSmbZgIHmdkU4ADgdELfpNW4+0RgIoR+RLmLvP6dckpoBXHVVfG8//HHh5HNL7sM9twTdtghnjhERCQncvYQ293vNbMvgTuBPyTtmg/cAnxS3fmF/FBaRKRar70WEtgOHcLrpk3hjTfSH9+0KZTVw1ARnTqlb45dojJ9RJAoDNum2Z/YviibNzezfYG7CM2mBrp7tQVlFW6I1gOyPC/vPfYYTJkCZ58NG24YXxxXXw233175UEpERMTMDgWeAp4D+gAto/U04G+Esj0td5/o7mXuXtYhcXMoIpLvFi+G42qYxGaLLcI0QS1bNkxM6cybFwaxSl0y6YNcpDKt8U0MXtEzzf4e0Tpd86nVmNlBwB2Emt7ByX2EsrAgWreqxbl5a9my0DqiVy84/fR4Y2naNPSthzBoXMuW0KVLvDGJiEit5OQhdtSP92bgTcI4G4n+wu+b2WGEJtUHmdlAd3+6ThGLiOSLadPgqKNg7ty4I5FayrTGd0a0Hmpmvzgnms5oO6AcyGiUZTMbRmge9SWwYy2TXggDcEANTaoK0UEHheb8zZrFHUnw00+w886h7315edzRiIhILeTqIfZQoClhmsHUQbIqgGejl1vWJkgRkbyzcmXoA9isGTz/fPrmwiXcjLgQZJT4uvvHwFSgOzAyZfcFhBrX25KnPzCz3mbWO/VaZnY4cCswFxhQU/NmM9ssmjNwte3A2Ojl7Zl8jkLRogVcckkYwTxfrLEGXHttGFH9yCOLfvovEZFilKuH2IlHsunaKCe2/1SbIEVE8sazz4YanyZN4NFHQ1/dbbdVM+IClc0wYCcQ+uJeY2YPmtnFZjadMKjUh8DolOPfi5afmdkgQvOoRoQCeLiZnZ+yjEq5zqnAvOg9J5jZFWb2CPA60B64iVB7XPDc4eijw1Re+WjPPeHii+Huu8MI7CIiUjhy+BD7uWh9YPQQmqTj+xJmXHAgT0szEZEaLFkSpjbZcUe48sqwrUePUDslBSvjUZ3d/WMzKwPGEKYe2h34CrgauMDdF2ZwmW5UJttHpjlmDmGU54QHgTWBzYDBQHPgW+Ax4CZ3fzjTz5DvJk+Gf/wDttoqTCeUj844A95+G845B/r1C82fRUSkYJwAvEh4iD2E8IC6H2GO33QPsQF+nlvD3V8xs1uA4cCr0XRGcwgJ9b7AGsB4d3+n/j6GiEg9mTEjNG+cMydMsXJa6uQyUqiyms7I3T8jFHSZHLvarMnuPgmYlOV7PkhIfovawoXh/1W/fnDMMXFHk54Z3HRTeOilkZ5FRApLjh5iAxxF6Mt7BLAL0Ab4AXie8FC62lGdRUTy0oQJcNJJsPHGoZmzbnaLSuzz+Epw1lnwzTfwxBP5Pw918+Zw7rnh5++/DwNfaTYKEZHCUNeH2NF2JzzInpSzwERE4rJqFTRuDLvsEmqixoyJfzoiybk8T7FKwxtvwI03hgdMffvGHU3mKipCk+z994fly+OORkREREQkC0uXwh//CP/3f2GwnZ494YorlPQWKSW+eWDTTUPz4TFj4o4kO40ahT6/zz8f+v9rpGcRERERKQjPPAObbRamLVlvvVDrK0VNiW/MVq4MLSuOOgratIk7muz9/vcwejTcfDNcfXXc0YiIiIiIVCNRyztwYBi85plnYPz4MGWRFDUlvjH64ovQd/7xx+OOpG7GjIF99w1dIp58Mu5oRERERETSWLoU7rkn9DH83/9ghx3ijkgaiBLfGI0aBfPnh+4EhaxRI7jtNjjsMNhkk7ijERERERFJsnQpjBsXmjN37AgffBCaKrZqFXdk0oCU+MbkscdgyhQ4+2zYcMO4o6m71q1h0iRYf/3wN2XJkrgjEhEREZGS9+yzsPnmoWni00+Hbe3axRmRxESJbwyWLYMTT4ReveD00+OOJrfc4aCDwkjPK1fGHY2IiIiIlKSlS+Hkk0Nf3ooKmDEDhgyJOyqJkRLfGDz0EHzyCVx/PTRrFnc0uWUGu+8e+voWW1IvIiIiIgXioIPgmmtg5Eh4882QAEtJ0/BlMTj44NAXdrPN4o6kfhx9NLz1Vug6semmYcRqEREREZF6VV4e1i1bwrnnhnk3lfBKRDW+Dcg91PRC8Sa9CVdeCTvvDMcfD889F3c0IiIiIlLUXngB+vaFs84Kr/v3V9Irv6DEtwFNngy9e8Mrr8QdSf1r0gTuvhu23RbWWCPuaERERESkKJWXw6mnhmmJVqyAffaJOyLJU2rq3EAWLgyDyW2xBZSVxR1Nw1hrrTCOgFl4vWoVNG4cb0wiIiIiUiRefz30IZw1C044AS69NEw1IlIFJb4N5Kyz4Jtv4PHHw7y3pSKR9I4eDe+9F6ZwKqXPLyIiIiL1pHVraNoUpk2DwYPjjkbynFKQBvDyy3DjjXDSSfDb38YdTTzWWQceeADOOy/uSERERESkYL34Ypg6xB169gwjqirplQwo8W0Ar70GXbvCmDFxRxKfUaPgyCPhr38NfX9FRERERDK2bFnoN7j99qEJ4YIFYbuaEkqG9JvSAE44ITTzbdMm7kjiYwbXXQfbbQeHHgrrrhv+TnXvHgb9EhERERGhc+dw45i6tGkD48bBsceGWt6OHeOOVAqMEt969MUX8NRT4ecWLeKNJR80awbDhoVBrubNCy1U5syBESOU/IqIiIgIMH9+1dtXrQo31tdfX9q1SVJrSnzr0SmnwF57wddfxx1J/rj00pDwJisvD4NfiYiIiIikNWRI3BFIAVPiW08efxzuvTckdGqJUWnu3Kq3z5kD55wDzz0XpmATERERERHJFSW+9WDZMhg5Enr1gj/9Ke5o8kvXrlVvX2MNuOgiGDAAXnopbJs9O/SNTq0hFhERERERyYYS33pw8cXwySdhMKdmzeKOJr+MHQstW/5yW8uWcPPN8O23cP/90K9f2P63v8Emm4Rk+cgj4c47KwfwExEREZEi89NPcUcgRUyJbz3o2jXU+GpKsdUNGwYTJ0K3bmGAvm7dwuthw6BdO9hvv1D7C2He4xtvhP79wxzAhxwCffpARUXY/8EHoXZdRERERIrA8cen39epU8PFIUXJvITakZaVlfnMmTPjDkNqYdWqMB/yZ5/BAQeEbT17htc77AA77wxDh8Kmm2o6NxFpOGb2mruXxR1HIVPZLCI/e+ih0M/tzDPjjkQKWLqyWSlCDk2ZAjfdVFkjKbnTuDFsvXVl0usO48eHqdy++ALOOAP69oUTT6zc/+WXcUUrIiIiIhlL3LTts4+SXqk3SnxzZOHC0Lz5H/+IO5LSYAa77x6S33fegc8/h0mTQnNogPffh/XXD32ER42CRx+FJUtiDFhEREREVvfYY7DhhmEtUo+U+ObI6NHwzTdhTm01tW14668Phx8O228fXq+9Nlx+OWywQegnvOeeYdvTT4f95eWh+bSIiIiIxOS11+Cgg+DXvw5910TqkVK0HHj5ZbjhhjAY029/G3c0AmH8g9NPhyeeCLXxTz4Jp5wCm28e9k+YAB06wIEHhsT4k09+ef7kydC9e3iI0b17eC0iIiIiOfLpp7DHHrDOOqFpXuvWcUckRa5J3AEUuooKOO44WHddGDMm7mikKs2bw047hSWhX78wgvTUqXDffWFbnz7w1ltw110wYkSoFQaYMye8hjD6tIiIiIjUweLFsNtuYfqiGTOgc+e4I5ISoMS3jho1gosuCj+3aRNvLJK5gQPD4h6mRXrySZg3LwyiNXp0ZdKbUF4etivxFREREamj1q3hD38IzZv79Ik7GikRSnzrwD0MsrTbbnFHIrVlBr17hyVh7tyqj023XUREREQyUFERRiTt2hXOOivuaKTEqI9vHfzhD3DhhXFHIbnWtWvV29u3b9g4RERERIrK6aeHAXG++CLuSKQEKfGtpccfh9tvjzsKqQ9jx0LLlr/ctsYacOWV4ef33oMVKxo+LhEREZGCNX48XHUVHHoorLde3NFICVLim4XkkX732iv0wz/jjLijklwbNgwmToRu3UJT6G7d4OabQw3/0qUwZAhsuSX85z9xRyoiIiJSAO67D049FfbfH8aNCzdYIg1MiW+GJk8OI/vOmRP69q5cGabJmTIl7sikPgwbBrNnh64os2dXDmrVqlWYq3nRIth2Wzj22PB7ICIiIiJVeP31cCO1zTahuWTjxnFHJCVKiW+Gqhrpd/nysF1Kyz77wLvvhgeX//hHGBjrs8/ijkpEREQkD22yCYwcCQ89BC1axB2NlDAlvhnSSL+SrHXr0Od35kw4/HDo0iVsX7o03rhERERE8sL8+aGJXPPm4aZpnXXijkhKnBLfDKUb6TfddikNffvCZZeFriqzZ4ffhwsvDK0BRERERErSkiWwxx6wyy6h35hIHlDim6GqRvpt2TJsF4HwQHOnneDcc0NC/MwzcUckIiIi0sBWroTf/x7++18455wwKqxIHtBvYoaqGul34sTKQY9EOneGu++Gf/8bfvwRBg6Eo47Sg04REREpEe5wwgnhZui662DPPeOOSORnTeIOoJAMG6ZEV2q2227wzjuhyfPSpXrQKSIiIiXimmvgppvgL38JU1+I5BElviL1oGVLuPji8OAT4KWXQhlw3XXQp0+8sYmIiIjUiwMPhO++g/PPjzsSkdWoLkqkHiXmZ583D/73P9h889DdZdmyeOMSERERyZl334VVq2D99eGCCypvgETyiBJfkQaw777w/vthrIe//hU23RSeeiruqERERETq6M03oX9/+POf445EpFpKfEUaSMeOcNttIeFt1CjMASwiIiJSsD77DHbfHdZcE0aNijsakWqpj69IAxsyJDwcbdw4vH7wwTDH+zHHaCAsERERKRCLFoWkd/FieP556NIl7ohEqqXbbJEYNG8OTZuGn++9F447DrbfHt56K964RERERDJy2GHwwQdw//2hD5dInlPiKxKz22+HW2+FWbPgt7+FM84I0yCJiIiI5K2zzw43MEOGxB2JSEaU+IrEzCw8NH3/fTjiCLj8cnjiibijEhEREanCq6+Gdb9+cPDB8cYikgUlviJ5on17+Pvf4Y03YL/9wrZ//Qu++CLWsERERESCG26ArbcONygiBUaJr0ie2XzzUAtcXg7Dh0OfPnDNNWF6PBEREZFY/OtfMHIk7Lkn7LZb3NGIZE2Jr0ieatkSXn4Ztt0WTj45TJH3+utxRyUiIiIl55VXQrPmLbaAu+6CJpoYRgqPEl+RPLbRRvDYY3DnnWGqvH79wlpERESkQXz/Pey1F3TqBI88Aq1axR2RSK0o8RXJc2bhIev778Ntt8EGG4TtF18M3bqFuX+7d4fJk2MNU0RERIpR27Zw5ZXhSXynTnFHI1JraqcgUiDatascPHHs2DCLQMKcOTBiRPh52LAGD01ERESKzbJl8M47UFYGhx4adzQidaYaX5ECNHHi6tvKy2H06IaPRURERIrMqlXhSfqAATBvXtzRiOREVomvmXUxs5vN7EszW25ms81svJmtleH5rcxsmJndYWbvm9lSM1tsZjPN7DQzW6Oaczcxs3vM7Gsz+9HMPjCzC8ysRTafQaQYpOvnO3duw8YhIiIiRcYdTjkFHngALrkEOneOOyKRnMg48TWzjYDXgOHAK8BVwCfAycB/zKx9BpfZAbgd2AV4G5gA3AGsD1wBzDCz5lW8dz/gVWBf4CngauAH4FzgSTNrlunnECkGXbtmt11EREQkI+PGwYQJcOqpcNJJcUcjkjPZ1PheB3QETnL3fd39THcfTEiAewFjM7jGPOBQYF13PzC6xrFAT+B1YFtgZPIJZtYYuAVoCRzo7oe4+5+BfsB9wHbAKVl8DpGCN3ZsmO4oWcuWYbuIiIhIrTz3HJx+Ohx0EFx+edzRiORURolvVNs7FJgNXJuy+zxgKXCYmVU7vrm7v+Huk939p5Tti4Ero5cDU07bEegDPOvuDyedUwGcEb08zswsk88iUgyGDQv9fLt1C6M+d+sWXu+9d9yRiYiISMHabrtQ23vrrWHaCJEikulv9KBoPTVKOH8WJa0vEGpk+9chlhXRemXK9sHR+vHUE9z9E+BDoBuwYR3eW6TgDBsGs2dDRUVYL1gAvXrBDz/EHZmIiIgUlPffDwOFNGoEJ54IzVfreShS8DJNfHtF6w/T7J8VrXvWIZYjo3VqgtsQ7y1S8LbfHr76Ss2dRUREJAtffQW77AL77RcGthIpUpkmvm2j9fdp9ie2t6tNEGZ2IrAr8AZwcy7f28xGRKNGz1ywYEFtwhMpCGVlcMQRMH48fPxx3NGIiIhI3lu8GPbYA779Fm66KfSfEilSsTfeN7P9gfGEga8OcPcV1Z+RHXef6O5l7l7WoUOHXF5aJO9cdBE0bRrGpRARERH5hc6dQ3KbWNZcE/7733DzsMUWcUcnUq8yTXwTtapt0+xPbF+UzZub2b7AXcDXwMCoz26DvLdIMVp3XTjrLHj8cZgzJ+5oRCQfmVkXM7vZzL40s+VmNtvMxpvZWrW41hZmdoeZfR5da76ZPWNmf6iP2EWkjubPr3r7okUNGoZIHDJNfD+I1un60faI1un64a7GzA4C7gXmAzu6+wdpDs35e4sUs1NPDWNUdOsWdyQikm+iWRpeA4YDrxCmJPwEOBn4j5m1z+JaJwKvEmZ9mEaYneEBoDGwe24jFxERqZsmGR43I1oPNbNGySM7m1kbwly65cBLmVzMzIYB/wS+AAalqelNmA6MJvQBvjjlOhsSEuI5hIJbpOQ1bx6SXnf44gvo0iXuiEQkj1wHdAROcvcJiY1mNg44BRgLHFfTRcxsKHAN8CRwYDTDQ/L+prkMWkREpK4yqvF194+BqUB3YGTK7guAVsBt7r40sdHMeptZ79RrmdnhwK3AXGBADUkvwDPAe8AAM/t5llIzawRcGr28wV3D0Ikk+8tfQned79MNCyciJSWq7R0KzAauTdl9HrAUOMzMWmVwucuBZcAhqUkvQK7H6xCRHNCtspS4TGt8AU4AXgSuMbMhhGS0H2GO3w8JtbLJ3ovWPw8PZ2aDCKM2NyLUIg+31UePW+Tu4xMv3H2VmQ0n1PxOMbMphKR5CFBGmEP4qiw+h0hJ+N3v4LLL4MIL4Yor4o5GRPLAoGg9NbnlFoC7LzazFwiJcX9C0+UqmdlvgM2AB4HvorJ9S8AJszPMSL2+iMTMPfSFEilhGSe+7v6xmZUBYwjNjncHvgKuBi5w94UZXKYblbXMR6Y5Zg5hlOfk937ZzLYi1C4PBdpEx40BLnH35Zl+DpFSscUWMHw4XHMNHHss9OhR8zkiUtR6Ret0Y2LMIpSxPakm8QW2itZfA08DA1L2v2Vm+7v7R7WMU0RyyR1GjQo3BC1bQnn56sd06tTgYYk0tKymM3L3z9x9uLuv6+5ruHs3dx9VVdLr7ubulrJtUmJ7NUv3NO/9rrsf5O7ruHszd+/p7ue5+7KsPrFICRk7NvT51fRGIkLlLAjpOkAktrer4Todo/VRhC5Qe0TX7gncDmwKPGpma6S7gJmNMLOZZjZzwYIFNUcuIrU3bx7cfXdIfpcsCYlw6jJvXtxRitS72OfxFZH607kzjB4NL7+cfgYDEZEsJe4dGgMHu/u/3f0Hd58F/AGYSUiCD0h3AXef6O5l7l7WoUOH+o9YpBQlktp11w1z9Y4bF+buFSlRSnxFityoUTBrlloxicjPNbpt0+xPbF9Uw3US++e5+3+Sd0QDTT4Uvdw6y/hEJFcqKmDkSPjznyuTXyW9UuKU+IoUuWbNoE0bWLEC3nor7mhEJEYfROueafYnRgJI1wc49TqL0uxPdH9qkVlYIpJTiaT3+uuV7IokUeIrUiKOOw4GDYKFmQxDJyLFaEa0HhpNCfgzM2sDbAeUAy/VcJ2XCFMfdU8z9dFvovWndYhVRGqjogKOPx5uuAHOPBMuuUTJr0hEia9IifjjH+G772DMmLgjEZE4uPvHwFTCgFQjU3ZfALQCbnP3pYmNZtbbzHqnXKcc+AfQHPirJc1LaGabAkcAK4Epuf8UIlKtE0+EiRPhrLPgoouU9IokyWYeXxEpYH37wtFHw9/+FqY36t27xlNEpPicALwIXGNmQ4D3gH6EOX4/BEanHP9etE69ez6HMI3RKGCbaA7gTsD+hIR4VJRoi0hDGjwYOnaE885T0iuSQjW+IiXkwguhRQtNbyRSqqJktAyYREh4TwM2Aq4G+rv7txle5wdgB+AiYG3gRGBP4HlgF3e/OufBi0jVVq2CV18NPx94IJx/vpJekSoo8RUpIZ06wTnnwGefwaJFcUcjInFw98/cfbi7r+vua7h7N3cf5e6rjQDg7ubuVd5Bu/sSdx/t7j3dvZm7t3P3oe4+tf4/hYgAIek96ijYdlt4//24oxHJa0p8RUrMqFHw+uvQrl3ckYiIiEitrVoFw4fDP/8ZnmqrD5NItZT4ipSYpk2hceMwuvNzz8UdjYiIiGRt1So4/HC47bbQj+ncc+OOSCTvKfEVKVHHHAP77htGehYREZECcvfdMHkyjB0LZ58ddzQiBUGJr0iJOu+80M/3ggvijkRERESy8n//B089FaYtEpGMKPEVKVGbbgojRsC118J779V8vIiIiMRo5cowT+/774dRm4cMiTsikYKixFekhI0ZA61bw2mnxR2JiIiIpLViBRxySHhaPX163NGIFCQlviIlrEOHMB5GkyZQXh53NCIiIrKaFStC0+Z774UrroATTog7IpGCpMRXpMSdcgo8/DC0bBl3JCIiIvILK1bAwQfDfffBuHFqoiVSB0p8RUqcWVh/9BE8+mi8sYiIiEiSn36Cb7+F8ePDk2oRqbUmcQcgIvlh1Ch44YWQALdvH3c0IiIiJeynn8LSunUYvbmJbtlF6ko1viICwKWXwuLFYZojERERicny5XDggbD77mEkZyW9IjmhxFdEAPj1r+G44+CGG+Cdd+KORkREpAQtXw4HHAD/+lcY0EpJr0jOKPEVkZ9dcAG0aRO6EbnHHY2IiEgJ+fFH2H//MODGDTfA8cfHHZFIUVHiKyI/a98exo6FTTYJratERESkgZxwAvz733DjjXDssXFHI1J01H5CRH5B0wOKiIjE4MwzYdAgOOywuCMRKUqq8RWRKj3zDNx5Z9xRiIiIFLFly0KzZnfo2VNJr0g9UuIrIlW65JLQvWjBgrgjERERKULl5bD33qGp1SuvxB2NSNFT4isiVbrySliyBM49N+5IREREikx5Oey1F0ybBrfcAv36xR2RSNFT4isiVdpkk/AQeuJEePPNuKMREREpEkuXwp57wowZ8M9/wuGHxx2RSElQ4isiaZ1/PrRrp+mNREREcmbmTPjPf+DWW9WnV6QBaVRnEUlr7bXhiivghx9C4msWd0QiIiIFKlGQ7rgjfPIJrLtu3BGJlBTV+IpItYYPh5NPhkb6ayEiIlI7S5bAkCFw993htZJekQanW1kRqZE7TJoEf/973JGIiIgUmMWLYbfd4Nln445EpKQp8RWRGpnBlClw2mnw9ddxRyMiIlIgfvgBdt019Om98074/e/jjkikZCnxFZGMXHllmH3h7LPjjkRERCQPde4cnhQnL23bwosvhibOBx0Ud4QiJU2Jr4hkpFcvOPHE0Nz5jTfijkZERCTPzJ+fft8BBzRcHCJSJSW+IpKxc88NIz2PGqXpjURERESkcGg6IxHJ2FprwbXXQosWcUciIiIiIpI5Jb4ikhWNyyEiIiIihUZNnUUkaxUVcN55MH583JGIiIiIiNRMia+IZK1RozDA1TnnwLx5cUcjIiKSBzp1ym67iDQoJb4iUitXXAHLl8Po0XFHIiIikgfmzQujQDZqBAsXhlEg3fWEWCRPKPEVkVrp0QNOOgluuQVefz3uaERERPLAIYfAP/8J7drFHYmIpFDiKyK1dvbZ0L49nHpq3JGIiIjkgV694NBD445CRKqgUZ1FpNbatQs1vl26xB2JiIhIzN56C959F/bZB5o3jzsaEUmhxFdE6mTPPSt/dgez+GIRERGJzT//CRMmhP69IpJ31NRZROpsxQr4wx/g0kvjjkRERCQm06fDtttCy5ZxRyIiVVDiKyJ11rQpLFkCf/0rfPll3NGIiIg0sG+/DfP8DR4cdyQikoYSXxHJicsvDzW/mt5IRERKztNPh/4+Q4bEHYmIpKHEV0RyYqONYNQomDQJZs6MOxoREZEGNHMmtG4NW20VdyQikoYSXxHJmdGjoWNH1fqKiEiJuegi+Oij0PdHRPKSRnUWkZxZc0245x7o2TPuSERERBqQGXTqFHcUIlIN1fiKSE7tuCOsu27o6rRyZdzRiIiI1LMHHoBhw2DRorgjEZFqKPEVkZxbtiwkwBdfHHckIiIi9eyhh+CJJ0KzJxHJW0p8RSTnWrQIfX0vuQS++CLuaEREROqJO0ybBoMGQSPdVovkM/0PFZF6cfnlsHw59OoV7gW6d4fJk+OOSkREJIc++gg+/1zTGIkUAA1uJSL14sUXw1gfS5eG13PmwIgR4edhw+KLS0REJGemTw/rwYPjjUNEaqQaXxGpF6NHrz64VXm5pjoSEZEi0qxZqO3t0SPuSESkBlklvmbWxcxuNrMvzWy5mc02s/FmtlYW19jZzK40s2lm9q2ZuZk9X8M5Xs3yUjafQUQaxty52W0XEREpOEccAU89FZo4iUhey7ips5ltBLwIdAQeAt4HtgZOBnY1s+3c/dsMLjUS2Af4EfgIWDvDEOYAk6rY/nmG54tIA+raNTRvrmq7iIhIwVu2DJo2hSbqOShSCLL5n3odIek9yd0nJDaa2TjgFGAscFwG17kUGE1InDcAPs3w/We7+/lZxCsiMRo7NvTpLS+v3NakSdguIiJS8K6/HsaMgU8/hbUybvwoIjHJqKlzVNs7FJgNXJuy+zxgKXCYmbWq6Vru/h93f8fdV2UZq4gUkGHDYOJE6NYttABr3RpWrYJNN407MhERkRyYNg06dVLSK1IgMu3jOyhaT3X3iuQd7r4YeAFoCfTPYWyp2pnZkWZ2lpmNNLP6fC8RyYFhw2D2bKioCM2e114bRo4M0x6KiIgUrBUr4NlnNY2RSAHJNPHtFa0/TLN/VrTuWbdwqrU58A9Ck+q/Af8xszfMTPVHIgVg7bXhkkvg+ec1n6+IiBS4V1+FJUs0jZFIAck08W0brb9Psz+xvV2doklvHLAd0AFoA2wFTCEkw9PNbP10J5rZCDObaWYzFyxYUE/hiUgmjjwStt4a7rsv7khERETqYNq00I9n0KCajxWRvFAQw9C5+2kpm2YCB5nZFOAA4HTCAFtVnTsRmAhQVlamBpYiMWrUCB55BNq3jzsSERGROthtN1hzTRVoIgUk08Q3UaPbNs3+xPZFdYomezcQEt8BDfy+IlJLHTqE9ddfh1ZiG24YbzwiIiJZKysLi4gUjEybOn8QrdP14e0RrdP1Aa4vibbLNY4mLSL5Y9Uq2HZbGD5cA12JiEiBmTULpk8PA1yJSMHINPGdEa2HmtkvzjGzNoT+t+XASzmMLROJkZ0/aeD3FZE6aNwY/vSnMCDmnXfGHY2IiEgWbr4ZdtkFli+POxIRyUJGia+7fwxMBboDI1N2X0Cocb3N3ZcmNppZbzPrXdcAzWwzM2ta1XbCCM8At9f1fUSkYR19NGy5JZx+OvzwQ9zRiIiIZGj6dOjXL0xQLyIFI5vBrU4AXgSuMbMhwHtAP8Icvx8Co1OOfy9aW/JGM9seODp6mfiL0cPMJiWOcfcjkk45FdjLzJ4DPgOWA72BXYHGwE2A6oxECkzjxnDttdC/P4wZA1dcEXdEIiIiNVi0CGbOhLPPjjsSEclSxomvu39sZmXAGELSuTvwFXA1cIG7L8zwUhsDh6ds65iy7Yiknx8E1gQ2AwYDzYFvgceAm9z94Uw/g4jkl3794KijYP780NfXrOZzREREYvPMM1BRofl7RQpQVtMZuftnwPAMj63yFtbdJwGTsnjPBwnJr4gUoRtvDLW/IiIiee/ZZ6FFi9BcSUQKSqaDW4mI1ItE0vvOO6HblIjUPzPrYmY3m9mXZrbczGab2XgzW6sO1xxgZqvMzM3sr7mMVyRvXHopvPYaNGsWdyQikqWsanxFROqDe2jy/Pnn8P77Gi9EpD6Z2UaEMTs6Ag8B7wNbAycDu5rZdu7+bZbXbAP8kzDDg/4HS/Fq0gT69Ik7ChGpBdX4ikjszOCqq+CLL+DCC+OORqToXUdIek9y933d/Ux3HwxcBfSicsaEbFwNtAUuzl2YInnmiSdg1ChNRSBSoJT4ikhe2GYbGD4cxo2D996r+XgRyV5U2zsUmA1cm7L7PGApcJiZtcrimvsQxv84CfgyN5GK5KEpU2DSJGiV8X8PEckjSnxFJG9ccklo5vzHP4bmzyKSc4Oi9VR3r0je4e6LgReAlkBGI/eYWUfCtIIPuvvtuQxUJO9MmwYDB2pERpECpcRXRPJGx44h+d16a1i5Mu5oRIpSr2j9YZr9s6J1zwyvdxPhXuK4ugQlkvc+/TQsQ4bEHYmI1JIGtxKRvHLssXFHIFLU2kbr79PsT2xvV9OFzOxIYG/g9+4+P5sgzGwEMAKga9eu2ZwqEo/EtAOav1ekYKnGV0Ty0uOPwzXXxB2FiFTFzLoD44F73f2ebM9394nuXubuZR06dMh1eCK5t2wZbLYZbLJJ3JGISC0p8RWRvHTXXXD66fDBB3FHIlJUEjW6bdPsT2xfVMN1bgaWASfkICaR/HfiifC//4VpCESkICnxFZG8dOml0LKlBroSybHEo6R0fXh7ROt0fYATtiBMibTAzDyxALdE+0dH2x6sU7Qi+WDVqrgjEJEcUB9fEclLnTrBmDFw8slw//1wwAFxRyRSFGZE66Fm1ih5ZGczawNsB5QDL9VwnVsJoz+n6gEMAN4AXgP+W9eARWJ33XVhsvnXX4d27eKORkRqSYmviOStE06Af/wDTjkFdt1VUyeK1JW7f2xmUwlz+Y4EJiTtvgBoBdzo7ksTG82sd3Tu+0nXOamq65vZEYTE91F3PzvnH0AkDtOmhSbOSnpFCpoSXxHJW02ahAft77wDzZvHHY1I0TgBeBG4xsyGAO8B/Qhz/H4IjE45/r1orc6NUnpWrYKnn4aDDoo7EhGpIyW+IpLXttsuLCKSG1GtbxkwBtgV2B34CrgauMDdF8YZn0heef11+P57zd8rUgSU+IpIQZg0CZ58Em6/XYNqitSVu38GDM/w2Iz/x7n7JGBS7aISyUOJ+XsHDYo3DhGpM43qLCIFYdEiuOMOeOihuCMREZGSsfXWcPbZYcRFESloSnxFpCCceCL85jcwahSUl8cdjYiIlIRBg+DCC+OOQkRyQImviBSEJk3g2mthzhy45JK4oxERkaL32Wfw1luaTF6kSCjxFZGCMWAADBsGl14K8+bFHY2IiBS1m26Cvn3D4FYiUvCU+IpIQbn8cnjkEejcOe5IRESkqE2fDmVlmr9XpEgo8RWRgrLuurDzzuHn5cvjjUVERIrUkiXw8suaxkikiCjxFZGCNGECbLopLFsWdyQiIlJ0nnsOVq6EwYPjjkREckSJr4gUpE03hVmzNNCViIjUg2nToFkz2G67uCMRkRxR4isiBWngQDj44DDQ1ccfxx2NiIgUlXPOCclvixZxRyIiOaLEV0QK1hVXQNOmcMopcUciIiJFpW1b1faKFBklviJSsNZfH847Dx57DD74IO5oRESkKDz7LIwdGwa4EpGiocRXRAraySfDW29Br15xRyIiIkXhrrvCABLNmsUdiYjkkBJfESloTZtC797h57lz441FRESKwLRpMGBAKGBEpGgo8RWRonD11aHW99NP445EREQK1uefw4cfav5ekSKkxFdEisIBB0DjxjBqVNyRiIhIwZo+PayV+IoUHSW+IlIUunSBc8+Fhx+Gf/877mhERKQgffVVKFA23TTuSEQkx5T4ikjRGDUq9Pc96ST48ce4oxERkYLz5z/D7NnQSLfIIsVG/6tFpGissQZMmADffQdvvx13NCIiUpAaN447AhGpB0p8RaSo7LQTzJkDZWVxRyIiIgXl73+Hfv1g0aK4IxGReqDEV0SKTps2UFEBTzwRdyQiIlIwnngi9PFt2zbuSESkHijxFZGidMstsOuu8PjjcUciIiJ5r6ICZsyAwYPBLO5oRKQeKPEVkaJ06KHQowf88Y+wfHnc0YiISF5780349ltNYyRSxJT4ikhRatYsDHT10Udw5ZVxRyMiInlt2rSwHjw43jhEpN4o8RWRorXLLrDffvDXv8LcuXFHIyIieatHDzj2WFh//bgjEZF6osRXRIraVVeF+5n58+OORERE8tbee8MNN8QdhYjUoyZxByAiUp+6dYM33tBYJSIiksaCBbBqFXTuHHckIlKPVOMrIkXPDJYuhcsvh59+ijsaERHJKzfeGJo4a/5ekaKmxFdESsILL8AZZ4SmzyIiIj+bNg023xzatYs7EhGpR0p8RaQkDB0K++wDF14In30WdzQiIpIXysvhxRc1jZFICVDiKyIlY/z40I3rtNPijkRERPLCiy+GPjCaxkik6CnxFZGS0b07nHUW3HsvPPVU3NGIiEjspk2DJk1ghx3ijkRE6plGdRaRkvKnP8GsWbDeenFHIiIisTvhBNhmG2jdOu5IRKSeKfEVkZLSvDncemvcUYiISF7YYIOwiEjRU1NnESlJX30FRxwBn38edyQiIhKLV1+FiRPDAFciUvSU+IpISVq2DO66C04/Pe5IREQkFrfdBqNGQePGcUciIg1Aia+IlKQNN4Qzz4S774YZM+KORkREGtz06WFQq2bN4o5ERBqAEl8RKVl//jOss06Y47dRozDq8+TJcUclIiL1bt48eOcdTWMkUkI0uJWIlKz774fFi2HlyvB6zhwYMSL8PGxYfHGJiEg9SzT1UeIrUjJU4ysiJWv0aFi+/JfbysvDdhERKWLvvQdrrQVbbBF3JCLSQJT4ikjJmjs3u+0iIlIkxowJzXw0sJVIyVDiKyIlq2vXqrevuSa4N2wsIiLSwNq0iTsCEWlAWSW+ZtbFzG42sy/NbLmZzTaz8Wa2VhbX2NnMrjSzaWb2rZm5mT2fwXmbmNk9Zva1mf1oZh+Y2QVm1iKbzyAikjB2LLRs+cttjRvD99/D8OHw00/xxCUiIvXojjtgzz3DH3sRKRkZD25lZhsBLwIdgYeA94GtgZOBXc1sO3f/NoNLjQT2AX4EPgLWzuC9+wHTgabAFOAzYDBwLjDEzIa4+/JqLiEisprEAFajR4fmzV27hmT4o4/g/PPDtvvuC93ARESkSDzyCLz2WmjeIyIlI5tRna8jJL0nufuExEYzGwecAowFjsvgOpcCowmJ8wbAp9UdbGaNgVuAlsA+7v5wtL0RcA9wQPT+l2TxWUREgJD8VjWC80Ybwdlnh1GflfiKiBQJ9zB/75AhYBZ3NCLSgDJq6hzV9g4FZgPXpuw+D1gKHGZmrWq6lrv/x93fcfdVGca4I9AHeDaR9EbXqQDOiF4eZ6a/XiKSO4ceCu+/H2qBKyrCAKAiIlLg3n0X5s8Pia+IlJRM+/gOitZTo4TzZ+6+GHiBUCPbP4exJSQmWHs8dYe7fwJ8CHQDNqyH9xaREta8eVhfdlmY8WLKlHjjERGROpo+Paw1f69Iyck08e0VrT9Ms39WtO5Zt3Dy7r1FRDjqqJD4HnQQXH65RnwWESlY7dvDAQdA9+5xRyIiDSzTxLdttE43/F1ie7s6RVMP721mI8xsppnNXLBgQa5jE5ES0KEDTJsGv/sdnHEGHH88rFwZd1QiIpK1Qw5R8x2RElX08/i6+0R3L3P3sg4dOsQdjogUqObN4c474S9/gUmT4O23445IRESy8sMPUF4edxQiEpNME99ErWrbNPsT2xfVKZr8e28RkZ81agQXXRQGuurbN2xbsiTWkEREJFPXXQdrrw2LFsUdiYjEINPE94Nona4fbY9ona4fbl3E+d4iIqv51a/C+o47oFevMB2kiIjkuenToUcPaNcu7khEJAaZJr4zovXQaP7cn5lZG2A7oBx4KYexJUTD77Fr6g4z25CQEM8BPqmH9xYRSWvzzaFJExgwAB5+uObjRUQkJsuXw/PPaxojkRKWUeLr7h8DU4HuwMiU3RcArYDb3H1pYqOZ9Taz3jmI8RngPWCAme2ddP1GwKXRyxvcNc6qiDSsX/8aXn4ZNtkE9t0Xrrkm7ohERKRKL70Ey5ZpGiOREtYki2NPAF4ErjGzIYRktB9hjt8PgdEpx78XrS15o5ltDxwdvWwdrXuY2aTEMe5+RNLPq8xsOKHmd4qZTQHmAkOAMsIcwldl8TlERHKmc2d45hkYNgxOPhnKymDbbeOOSkREfmHatDBQw447xh2JiMQk48TX3T82szJgDKHZ8e7AV8DVwAXuvjDDS20MHJ6yrWPKtiNS3vtlM9uKULs8FGhDaN48BrjE3Zdn+jlERHKtZcswO8YTT1Qmve5gVv15IiLSQH7/+zBAQ9t0Y6WKSLHLpsYXd/8MGJ7hsVXe8rn7JGBSNu8bnfcucFC254mINITGjWH33cPPr74Kp5wC99wD660Xb1wiIkLom/LrX8cdhYjEqOjn8RURaWjffQf/+x/06wdvvhl3NCIiJe7tt+GBB8IAVyJSspT4iojk2C67wHPPhebO228Pjz8ed0QiIiXsllvg4IOhoiLuSEQkRkp8RUTqQd++YRDRDTeEPfcMibCIiMRg+vQwAEOLFnFHIiIxUuIrIlJPunQJCe8FF8A228QdjYhICfrmG3jjDc3fKyJKfEVE6lObNjB6NDRpAl9+CSNHQnl53FGJiJSIp58OayW+IiVPia+ISAN5+mm4/noYPBjmz487GhGREvDSS9C6dZhkXURKmhJfEZEGcsghcP/9YaTn/v3h3XfjjkhEpMhdfnn4Y9u0adyRiEjMlPiKiDSgffeFZ5+FZcvCWCuvvBJ3RCIiRcwMNtgg7ihEJA8o8RURaWBlZfDyy7DTTtCzZ9zRSKkxsy5mdrOZfWlmy81stpmNN7O1Mjy/lZkNM7M7zOx9M1tqZovNbKaZnWZma9T3ZxDJyIMPwvDh8MMPcUciInlAia+ISAy6dYMpU6BdO/jxR5g4Mcz7K1KfzGwj4DVgOPAKcBXwCXAy8B8za5/BZXYAbgd2Ad4GJgB3AOsDVwAzzKx57qMXydIDD8Ajj4Q+viJS8pT4iojEbPJkOPZYGDYsJMEi9eg6oCNwkrvv6+5nuvtgQgLcCxibwTXmAYcC67r7gdE1jgV6Aq8D2wIj6yd8kQy5h/l7Bw2CRrrdFRElviIisTvySLj4YrjzTth55zDtpEiuRbW9Q4HZwLUpu88DlgKHmVmr6q7j7m+4+2R3/yll+2LgyujlwFzELFJrs2bB559rGiMR+ZkSXxGRmJnBmWfC3XfDq6/CNtuEezaRHBsUrae6e0XyjihpfQFoCfSvw3usiNYr63ANkbqbNi2sBw+ONw4RyRtKfEVE8sTvfgczZoRWeatWxR2NFKFe0frDNPsTj1vqMuTakdH68eoOMrMR0WBYMxcsWFCHtxNJo3FjGDAANt447khEJE8o8RURySPbbBOmnOzdO3RRe+mluCOSItI2Wn+fZn9ie7vaXNzMTgR2Bd4Abq7uWHef6O5l7l7WoUOH2rydSPVGjIBnnglNakREUOIrIpJ3GjcO69tvD4nwgQeGUaAbNYLu3cNgWCL5xMz2B8YTBr46wN1XVH+GSD368UeoqKj5OBEpKUp8RUTy1O9+B9ttB/fdB3PnhhrgOXNCRYaSX6mFRI1u2zT7E9sXZXNRM9sXuAv4Ghjo7p/UJjiRnLnuOujUCb5P17hBREqREl8RkTzVrBl89tnq28vLYfToho9HCt4H0TpdH94e0TpdH+DVmNlBwL3AfGBHd/+ghlNE6t+0adC+PbRN94xHREqREl8RkTxWVeILoQZYJEszovVQM/tF+W9mbYDtgHIgo57lZjYMuBP4kpD0aixyid+KFfDss5rGSERWo8RXRCSPde1a9fa11oL33mvYWKSwufvHwFSgOzAyZfcFQCvgNndfmthoZr3NrHfqtczscOBWYC4wQM2bJW+8+iosWaJpjERkNU3iDkBERNIbOzb06S0vr9zWogX89BP07RuaPJ95JqyxRmwhSmE5AXgRuMbMhgDvAf0Ic/x+CKQ2ok88Xvl5aFwzG0QYtbkRoRZ5uK0+cu4idx+f6+BFajRtWhjJeeDAuCMRkTyjxFdEJI8NGxbWo0eH5s1du4ZkeKedYNQoOO88uPde+PvfoV+/WEOVAuDuH5tZGTCGMPXQ7sBXwNXABe6+MIPLdKOyxdiRaY6ZQxjlWaRh7bxzeDrYvn3ckYhInjF3jzuGBlNWVuYzZ86MOwwRkZx55BE4/nj47ruQGOter2GZ2WvuXhZ3HIVMZbOIiORSurJZfXxFRArYnnvCO+/AAw9UJr2vvRZvTCIisfjoI3jhBVi5Mu5IRCQPKfEVESlwa64JQ4eGnx99FMrK4LDD4Jtv4o1LRKRB/f3voW/vjz/GHYmI5CElviIiRWSnneDcc+Huu6FPH7jjDiihHi0iUsqmT4f+/aF167gjEZE8pMRXRKSINGsGF1wAr78OG20UBscaPjzuqERE6tmiRaGfh6YxEpE0NKqziEgR+s1vQle3v/0N1lsvbFu1Kszy0UiPPEWk2DzzDFRUwJAhcUciInlKtz8iIkWqcWM4+WQ46KDw+pprYPvt4d13441LRCTnZswI0xj17x93JCKSp5T4ioiUiHXXhQ8/hL59Q3Po5cvjjkhEJEcuvRReegnWWCPuSEQkTynxFREpEQcfHGp7DzoIzj8fttgCNH2qiBSFZs1gs83ijkJE8pgSXxGREtKxI0yeDI88AkuXwrJlcUckIlJHU6fCn/8MixfHHYmI5DElviIiJWiPPUKz5x12CK8vvhgeeyzemEQkJp07h5HvUpfOneOOLDP33AMTJ0LLlnFHIiJ5TImviEiJSnSF+/HHMN/v7rvDoYfCggXxxiUiDWz+/Oy255tp02DgwDCin4hIGkp8RURKXPPmoa/veeeFipM+feD228E97shEpN4VevPgTz+F2bM1jZGI1EiJr4iI0KxZGPDqv/+FHj3g6KPh88/jjkpE6kXiqdaMGbDOOtUfO2pUfs+BNn16WA8eHG8cIpL3lPiKiMjPfv1reP55eOEF2GCDcH/88MOwalXckYlIrbnDG2/AmDFQVgbjx4ftv/0tjBxZ/bnXXRf+MOywQ2gKkm8j4v3wA2yySWiqIiJSDSW+IiLyC40bw5Zbhp+ffRb22Qe23x7eeSfeuEQkS+6hxrZbt5Dknn9+6NyfGLSqXTsYN676a3zxBVx2GcybB4cdFkZPziennAJvvx0G4xIRqYYSXxERSWvAgFDJM2tWuG8+7zxYvjzuqESkSgsXhpHqxowJr83Cf94ttoB//AO++gpefBH+7/9+eV6nTlVfr1Mn6NAB/vQn+OCDMIhUoob45ZfDH4jJk8MIeXFINNlW0isiGVDiKyIiaZnBsGHw3nvw+9+H++lddtHAVyJ5Y+7c0HR58OCQpA4bFqb2STyheuQRePBBOPLI9AnuvHnhP3XqMm9e5TGNGoX36NUrvP7uO/jyyzAU/Prrw6mnwvvv1+cnXd2ECdC7N3z/fcO+r4gUJCW+IiJSow4d4Lbb4N//Dve3ZqHfb6EPCCtScCoqQm3rDz+E11OmhOa+X38dmiG/9FJIhps1C/vrqzZ0t93CZOBPPRVGVJ4wAbbdFn76KexviKdj06bBihXQtm39v5eIFDwlviIikrHddoO99w4/X311GFPm0UdDa8fu3UOlUPfu4bWI5Eh5OfzrX3DMMbDeetC/f/iPB/CHP8BHH4V+rmPHQr9+4T9iQ2jUKCS999wThoG/557Qh9gdttsOTjstNJGuDytXwtNPaxojEclYk7gDEBGRwrTttnDzzbDnnmFArMTIz3PmwIgR4edhw+KLT6SgrVoV/mN99RVstFEYTXnNNSufPu22WzhunXVqnpKoIXTqVNmU+ocfQvPna64Jg2ftuCMceyzsv39lTXRdvf56eB9NYyQiGVKNr4iI1Er//uHes23b1ac7Ki+H0aPjiUukILmH+XIvuSQ8VTr88LB93XXhjDPgySdhwQK46y445JAwInO+atsW7r031AJffDF89lmI+ZFHwv5cNIOeNi2sBw2q+7VEpCQo8RURkVpbY43Kroap5swJLS+nTg2DzYqUpM6dQz/b1CUxpRDAFVdAjx5hvty//CX0k91ii8r9558PO+0U/sMVkk6d4Mwzw8jSU6dW9pO48MKQsN51V+2Hid9yy9CnOd2AXSIiKdTUWURE6qRr15DkpmrSBM4+u/J1jx4wfTp06QLffAMtW4ZFpKjNn59++4oV0LQpLFoU/oOcfnroO9ClS4OGWO8aNYKdd6583blz+KPxf/8XmmkfcUToH9GjR+bXHDo0LCIiGVKNr4iI1MnYsasnsC1bwqRJoab3qafgoougb9/KSq7zzw/dFfv2Dfe7f/87/O9/miZJioj76n0AUn38cVj/9a/w2GNw3HHFl/RWZcSIMCDXE0+EuYCvuirMFZywcmX153/xRRhRWn8wRCQLqvEVEZE6SQxgNXp0mEWla9eQDCe2Dxmy+sCrv/td6KL46quhK+BNN4X7/c8+C/snTQqtOrfeOozrU18zsojkxHffhblyZ80KCV1i/c9/Vn9e794NEl5eatSostb2q69gyZKw/aOPwojQhx8eEuSNN1793BtuCE/TFi4MT9BERDKgxFdEROps2LDsRnAeMCAsECptPvoo3PsmXHopvP9++HmttaCsDA44IAwMK9LgVqwI8+POmvXL5Pb448Mv5XffwVFHhfb9G24YmuwOHBjm9pKarbtu5c8rVoTEd9w4uPzyMGrzscfCSSet3my8bdvQx3fevIaNV0QKkhJfERGJlVnIE5K79731FrzzDrzySqgVfuWV8BpC69Hf/Ab69IGttgq1wmVl4R5YpNbmzftlUjtrVkjARo0KTW8TT2oSye3GG1dOI/SrX4Vmy127hv1Se336wP33w5dfwi23hOYghx0WBvyqSro+1CIiKfTXWURE8k6TJrD55mE55piwLdGdb/HiMODtK6/AAw9UnnPVVSFHKS+HN98M/YebN6/cP3ly+ubYUqA6d6468amqFtAdvv76l7W2HTqEXxqATTcNo65B+AX81a9gs83C6xYtwshs3bpVndw2bhyS4ap06pQ+RklvvfXCf9gzz4S33w7/oUVE6kCJr4iIFIREP9927UISC6GF6cyZoVZ4u+3CtpdeCn2KmzQJectWW4Va4smTYdmycMycOaH7ICj5LWjVjZg8aRL8+GMYMApghx3ghRcqj2ncOIygnEh8r7029Bft0SMkuKnJbW3ni1Uz3Lpp3Dg8ARMRqSPzEhoRr6yszGfOnBl3GCIiUo8WLoSnn65sJj1zJnz/fdXHdusGs2fX/r3M7DV3L6v9FaROZXNNo56tu25oMguh2eySJSGx3Xjj8I/ftGnt3lcaXnX/1iV0LysiNUtXNqvGV0REispaa8F++4UFoKIiVN5VdW88d27DxiYNaNaskNwmDB8eXywiIhI7zeMrIiJFrVGj0C2zKum2SxHYeGPV6BaTdH2i1VdaRDKkxFdERIre2LHQsuUvt7VsGbaLSAGYNy8020hd1IdaRDKUVeJrZl3M7GYz+9LMlpvZbDMbb2ZrZXmdtaPzZkfX+TK6bpc0x882M0+z6C+eiIhUa9gwmDgxtHw1C+uJEzWwVcFTLaCIiGQo4z6+ZrYR8CLQEXgIeB/YGjgZ2NXMtnP3bzO4TvvoOj2B6cBdQG9gOLCHmW3j7p9Ucer3wPgqti/J9DOIiEjpGjZMiW7RUW2fiIhkKJvBra4jJL0nufuExEYzGwecAowFjsvgOhcRkt5x7n5a0nVOAq6O3mfXKs5b5O7nZxGviIiIiIiISGZNnaPa3qHAbODalN3nAUuBw8ysVQ3XaQ0cFh1/fsruvwFzgF3MLM0s8CIiIiIiIiLZybSPb2LW9qnuXpG8w90XAy8ALYH+NVynP9ACeCE6L/k6FcATKe+XrJmZHWpmZ5nZyWY2yMwaZxi/iIiIiIiIlKhMmzr3itYfptk/i1Aj3BOYVsfrEF0nVWfgtpRtn5rZcHd/Jt0bmtkIYARAV81bISIiIiIiUnIyrfFtG62/T7M/sb1dPV3nFmAIIfltBWwK3Ah0Bx4zs83TvaG7T3T3Mncv69ChQw3hiYiIiIiISLHJZnCr2Lj7BSmb3gaOM7MlwGmE/sL7NXRcIiIiIiIikv8yrfFN1MS2TbM/sX1RA10n4YZoPSDD40VERERERKTEZJr4fhCtq+p7C9AjWqfru5vr6yQsiNbVjiYtIiIiIiIipSvTxHdGtB5qZr84x8zaANsB5cBLNVznJWAZsF10XvJ1GhEGyEp+v5okRpH+JMPjRUREREREpMRklPi6+8fAVMJgUiNTdl9AqHG9zd2XJjaaWW8z651ynSWEkZlbsfo8vidG13/C3X9OZM2sT1XzA5tZd8LcvwC3Z/I5REREREREpPSYu2d2oNlGwItAR+Ah4D2gH2HO3Q+Bbd3926TjHcDdLeU67aPr9ASmA68AfYB9gK+j63ycdPz5hAGsngXmAIuBjYA9gObAv4H93P2nDD7DgugadbUO8E0OrlPq9D3mhr7H3ND3mBul9j12c3dNGVAHKpvzjr7H3ND3mBv6HnOj1L7HKsvmjBNfADPbABgD7Aq0B74CHgAucPeFKcdWmfhG+9YGzgP2BdYFvgUeA851989Tjt0ROA74LZXTGS0C3iDUHt/m2XyIHDCzme5e1pDvWYz0PeaGvsfc0PeYG/oeJS763csNfY+5oe8xN/Q95oa+xyCr6Yzc/TNgeIbHrpbwJu37Djg5Wmq6zjPAM5nGKCIiIiIiIpIs08GtRERERERERAqSEt/amRh3AEVC32Nu6HvMDX2PuaHvUeKi373c0PeYG/oec0PfY27oeyTLPr4iIiIiIiIihUY1viIiIiIiIlLUlPiKiIiIiIhIUVPimyEz62JmN5vZl2a23Mxmm9l4M1sr7tgKhZm1N7OjzewBM/vIzJaZ2fdm9ryZHWVm+n2sJTM71Mw8Wo6OO55CYmZDot/JedH/7S/N7Akz2z3u2AqFme1hZlPN7PPo//UnZnavmW0Td2xS3FQ2143K5fqlsrn2VDbXncrm1amPbwbMbCPgRaAj8BDwPrA1MAj4ANjO3b+NL8LCYGbHAdcT5n+eAcwFOgH7A22B+4CDGnpe5kIXza/9FtAYaA0c4+5/jzeqwmBmlwF/Aj4nzCX+DdAB2BJ4yt3PiDG8gmBmlwJnEOZjf5DwHW4M7E2YMu8P7n57bAFK0VLZXHcql+uPyubaU9lcdyqbq6bENwNm9gQwFDjJ3SckbR8HnALc6O7HxRVfoTCzwUAr4FF3r0ja3hl4BdgAONDd74spxIJjZgY8CfwKuB84HRWuGTGzYwijHP4TGOHuP6Xsb+ruK2IJrkBE/3e/ABYAm7n710n7BgHTgU/dfcOYQpQiprK57lQu1w+VzbWnsrnuVDanpyYsNYieKA8FZgPXpuw+D1gKHGZmrRo4tILj7tPd/V/JhWu0fR5wQ/RyYIMHVthOAgYDwwm/i5IBM2sGjCXUbqxWsAKoYM1IN0I58nJywQrg7jOAxYSn9CI5pbI5N1Qu1xuVzbWgsjlnVDanocS3ZoOi9dQqCobFwAtAS6B/QwdWZBJ/yFbGGkUBMbM+wCXA1e7+bNzxFJidCX/07wcqon4wfzazk0u570stzAJ+ArY2s3WSd5jZAKAN8FQcgUnRU9lc/1Qu14LK5jpR2ZwbKpvTaBJ3AAWgV7T+MM3+WYSnzj2BaQ0SUZExsybAH6KXj8cZS6GIvrPbCE9Fz4o5nEK0VbT+Efgv8JvknWb2LKF534KGDqyQuPt3ZvZnYBzwrpk9SOhPtBGhH9GTwLHxRShFTGVzPVK5XDsqm+tMZXMOqGxOT4lvzdpG6+/T7E9sb1f/oRStSwh/3P7t7k/EHUyBOBf4LbC9uy+LO5gC1DFa/wl4F9gBeIPQH+sKwg3zvaiJX43cfbyZzQZuBo5J2vURMCm1mZVIjqhsrl8ql2tHZXPdqGzOEZXNVVNTZ4mVmZ0EnEYYjfOwmMMpCGbWj/Ak+Up3/0/c8RSoxN++lcDe7v68uy9x97eA/QgjSe6oplU1M7MzgCnAJMLT5FaEkTc/ASZHo3OKSIFQuVw7KptzQmVzjqhsrpoS35olnhq3TbM/sX1R/YdSXMzsROBqwlO9Qe7+Xcwh5b2oGdWthOZ958QcTiFbFK3/6+6zk3e4ezmQqOHYugFjKjhmNhC4FHjY3U9190/cvdzdXyfcpHwBnGZmJTdypNQ7lc31QOVy7ahszplF0Vplcx2obE5PiW/NPojWPdPs7xGt0/UzkiqY2ShgAvA2oXCdF29EBaM14XexD/CjmXliIYxkCnBTtG18XEEWgMT/60Vp9i+M1i3qP5SCtme0npG6I7pJeYVQzvy2IYOSkqCyOcdULteJyubcUNmcGyqb01Af35olfmmGmlmjlHnu2gDbAeXAS3EEV4iiDveXEPpt7Ozu38QbUUFZDvwjzb4tCH/EnicUHmpqld40wIFNUv9fRxIDanzasGEVnGbROt20CIntq01JIVJHKptzSOVynalszg2Vzbmhsjkdd9dSw0JoWuHAH1O2j4u23xB3jIWyEJoAOTATWDvueIppAc6Pvtuj446lEBbgoej7OiVl+1CggvBkuW3ccebzAvwu+g7nAeun7Nst+h6XAe3jjlVL8S0qm3P2Papcrt/vV2Vzdt+Xyua6f4cqm9MsqvHNzAnAi8A1ZjYEeA/oR5hH8ENgdIyxFQwzOxwYA6wCngNOMrPUw2a7+6QGDk1K00jCU/hxZrYHYeqEXwH7En5Hj3b3dCPGSjCFMBfgTsB7ZvYAoaDtQ2hqZcCZ7v5tfCFKEVPZXEcqlyUPqWyuO5XNaSjxzYC7f2xmZYTCYVdgd+ArwgAQF7j7wurOl5/9Klo3BkalOeYZwgh0IvXK3T83sy0J00/sDQwAfgD+BVzs7q/EGV8hcPcKM9udcKNyMGHQjJbAd8C/gWvcfWqMIUoRU9mcEyqXJa+obK47lc3pWVTtLSIiIiIiIlKUNKqziIiIiIiIFDUlviIiIiIiIlLUlPiKiIiIiIhIUVPiKyIiIiIiIkVNia+IiIiIiIgUNSW+IiIiIiIiUtSU+IqIiIiIiEhRU+IrIiIiIiIiRU2Jr4iIiIiIiBQ1Jb4iIiIiIiJS1P4fGZZ8esMTz60AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1152x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams[\"font.size\"] = 20\n",
    "fig=plt.subplots(figsize=(16,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('LOSS PLOT')\n",
    "plt.plot(cnn1.log_loss,'bo--')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('ACCURACY PLOT')\n",
    "plt.plot(cnn1.log_acc,'rs--');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AlTSrmTDxap"
   },
   "source": [
    "#Problem 8 - LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nF1f4hXxDwsR",
    "outputId": "3a7dbe0b-9ff4-4461-ac0a-572924966bc4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-110-be33727ef1fa>:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
      "<ipython-input-110-be33727ef1fa>:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
      "<ipython-input-110-be33727ef1fa>:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
      "<ipython-input-110-be33727ef1fa>:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
      "<ipython-input-110-be33727ef1fa>:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
      "<ipython-input-110-be33727ef1fa>:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
      "<ipython-input-110-be33727ef1fa>:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
      "<ipython-input-110-be33727ef1fa>:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
      "<ipython-input-110-be33727ef1fa>:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
      "<ipython-input-110-be33727ef1fa>:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "LeNetCNN = {0:SimpleConv2d(F=6, C=1, FH=5, FW=5, P=2, S=1,\n",
    "                      initializer=SimpleInitializerConv2d(),\n",
    "                      optimizer=SGD(),\n",
    "                      activation=ReLU()),\n",
    "            1:MaxPool2D(P=2),\n",
    "            2:SimpleConv2d(F=16, C=6, FH=5, FW=5, P=2, S=1,\n",
    "                      initializer=SimpleInitializerConv2d(),\n",
    "                      optimizer=SGD(),\n",
    "                      activation=ReLU()),\n",
    "            3:MaxPool2D(P=2),}\n",
    "\n",
    "LeNetNN = {0:FC(784, 120, HeInitializer(), AdaGrad(0.01), ReLU()),\n",
    "            1:FC(120, 84, HeInitializer(), AdaGrad(0.01), ReLU()),\n",
    "            2:FC(84, 10, SimpleInitializer(0.01), AdaGrad(0.01), Softmax()),}\n",
    "\n",
    "\n",
    "LeNet = Scratch2dCNNClassifier(NN=LeNetNN,CNN=LeNetCNN,\n",
    "                               n_epoch=10,n_batch=20,verbose=False)\n",
    "\n",
    "LeNet.fit(x_train[0:1000],y_train[0:1000])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ath-G4AVGLxS"
   },
   "outputs": [],
   "source": [
    "y_pred_LeNet = LeNet.predict(x_valid[0:100])\n",
    "\n",
    "accuracy = accuracy_score(np.argmax(y_valid[0:1000],axis=1), y_pred_LeNet)\n",
    "print('accuracy:{:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HOps5-BTG6zu"
   },
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 20\n",
    "fig=plt.subplots(figsize=(16,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('LOSS PLOT')\n",
    "plt.plot(LeNet.log_loss,'bo--')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('ACCURACY PLOT')\n",
    "plt.plot(LeNet.log_acc,'rs--');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lP-Da3pJ0tA"
   },
   "source": [
    "#Problem 9 - Survey of Well-Known image recognition models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qbpC2poJzub"
   },
   "outputs": [],
   "source": [
    "#Introduction\n",
    "#The human brain can easily recognize and distinguish the objects in an image. For instance, given the image of a cat and dog, within nanoseconds, we distinguish the two and our brain perceives this difference. In case a machine mimics this behavior, it is as close to Artificial Intelligence we can get. Subsequently, the field of Computer Vision aims to mimic the human vision system â€“ and there have been numerous milestones that have broken the barriers in this regard.\n",
    "\"The rapid developments in Computer Vision, and by extension â€“ image classification has been further accelerated by the advent of Transfer Learning. To put it simply, Transfer learning allows us to use a pre-existing model, trained on a huge dataset, for our own tasks. Consequently, reducing the cost of training new deep learning models and since the datasets have been vetted, we can be assured of the quality.In Image Classification, there are some very popular datasets that are used across research, industry, and hackathons. The following are some of the prominent ones:\"\n",
    "â€¢\tImageNet\n",
    "â€¢\tCIFAR\n",
    "â€¢\tMNIST\n",
    "\n",
    "\n",
    "##Pre-Trained Models for Image Classification \n",
    "â€¢\tVGG-16\n",
    "â€¢\tInceptionv3\n",
    "â€¢\tResNet50\n",
    "â€¢\tEfficientNet\n",
    "\n",
    "#1. Very Deep Convolutional Networks for Large-Scale Image Recognition (VGG-16)\n",
    "\"The VGG-16 is one of the most popular pre-trained models for image classification. Introduced in the famous ILSVRC 2014 Conference, it was and remains THE model to beat even today. Developed at the Visual Graphics Group at the University of Oxford, VGG-16 beat the then standard of AlexNet and was quickly adopted by researchers and the industry for their image Classification Tasks.\"\n",
    "\n",
    "#2. Inceptionv3\n",
    "\"While researching for this article â€“ one thing was clear. The year 2014 has been iconic in terms of the development of really popular pre-trained models for Image Classification. While the above VGG-16 secured the 2nd rank in that yearsâ€™ ILSVRC, the 1st rank was secured by none other than Google â€“ via its model GoogLeNet or Inception as it is now later called as.The original paper proposed the Inceptionv1 Model. At only 7 million parameters, it was much smaller than the then prevalent models like VGG and AlexNet. Adding to it a lower error rate, you can see why it was a breakthrough model. Not only this, but the major innovation in this paper was also another breakthrough â€“ the Inception Module.\"\"\n",
    "\n",
    "#3. ResNet50\n",
    "\"Just like Inceptionv3, ResNet50 is not the first model coming from the ResNet family. The original model was called the Residual net or ResNet and was another milestone in the CV domain back in 2015.The main motivation behind this model was to avoid poor accuracy as the model went on to become deeper. Additionally, if you are familiar with Gradient Descent, you would have come across the Vanishing Gradient issue â€“ the ResNet model aimed to tackle this issue as well. Here is the architecture of the earliest variant: ResNet34(ResNet50 also follows a similar technique with just more layers)\"\n",
    "\n",
    "#4. EfficientNet\n",
    "\"We finally come to the latest model amongst these 4 that have caused waves in this domain and of course, it is from Google. In EfficientNet, the authors propose a new Scaling method called Compound Scaling. The long and short of it is this: The earlier models like ResNet follow the conventional approach of scaling the dimensions arbitrarily and by adding up more and more layers.However, the paper proposes that if we scale the dimensions by a fixed amount at the same time and do so uniformly, we achieve much better performance. The scaling coefficients can be in fact decided by the user.Though this scaling technique can be used for any CNN-based model, the authors started off with their own baseline model called EfficientNetB0:\"\"\n",
    "\n",
    "##https://www.analyticsvidhya.com/blog/2020/08/top-4-pre-trained-models-for-image-classification-with-python-code/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExdWthxyNtrb"
   },
   "source": [
    "#Problem 11 - Survey on Filter Size\n",
    "\n",
    "1. why are 3*3 filters commonly used instead of larger ones such as 7**7?\n",
    "- Using a 3x3 filter expresses most information about the image across all the channels while keeping the size of the convolutions layers consistent with the size of the image (zero padding allows us to achieve this).\n",
    "\n",
    "\n",
    "\n",
    "2. What is the effect of a 1*1 filter with no height or width direction?\n",
    "- A 1 x 1 Convolution is a convolution with some special properties in that it can be used for dimensionality reduction, efficient low dimensional embeddings, and applying non-linearity after convolutions. It maps an input pixel with all its channels to an output pixel which can be squeezed to a desired output depth."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
